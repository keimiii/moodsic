{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef119572",
   "metadata": {},
   "source": [
    "# Generic Vision Backbone â†’ Fastai Regression Head (Valenceâ€“Arousal)\n",
    "\n",
    "This notebook provides a template for training a small regression head on top of a **frozen, pretrained vision backbone** for **valenceâ€“arousal (V-A)** prediction. ðŸ§ \n",
    "\n",
    "It's designed to be flexible and supports loading backbones from both **Hugging Face Transformers** (like `DINOv3`) and the **`timm` library** (like `ResNet` or `EfficientNet`), allowing for easy experimentation.\n",
    "\n",
    "* Targets are expected in **FindingEmo** units: $V \\in [-3,3]$, $A \\in [0,6]$.\n",
    "* For training, we map targets to a bipolar space: $v_{ref} = v/3$ and $a_{ref} = (aâˆ’3)/3$, which maps both to the range $[-1,1]$.\n",
    "* We report **Mean Absolute Error (MAE)**, **Concordance Correlation Coefficient (CCC)**, and **Spearmanâ€™s Ï** on the test set.\n",
    "* The backbone remains **frozen**; only the lightweight regression head is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install latest libraries (uncomment if needed) ---\n",
    "# %pip install -U torch torchvision torchaudio\n",
    "# %pip install -U fastai transformers timm torchmetrics datasets\n",
    "# %pip install -U accelerate\n",
    "#\n",
    "# If you're on Apple Silicon and want MPS acceleration, make sure your PyTorch build supports MPS.\n",
    "# See: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788dbf5",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54646240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, math, random, shutil, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import timm\n",
    "\n",
    "# Fastai\n",
    "from fastai.vision.all import *\n",
    "from fastai.learner import load_learner\n",
    "from fastai.callback.tracker import CSVLogger, EarlyStoppingCallback\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"image file is truncated\")\n",
    "\n",
    "# Metrics\n",
    "from torchmetrics.functional.regression import concordance_corrcoef as ccc_fn\n",
    "# (Spearman is computed later in the test block)\n",
    "\n",
    "# Device (prefers Apple MPS on M-series Macs)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set HuggingFace token for authentication\n",
    "if \"HUGGINGFACE_TOKEN\" in os.environ:\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ac104",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabb37d",
   "metadata": {},
   "source": [
    "## Model Selection (HF or timm)\n",
    "\n",
    "- Source: set `BACKBONE_SRC` to `\\\"hf\\\"` (HuggingFace Transformers) or `\\\"timm\\\"` (timm/torchvision).\n",
    "- Name: set `MODEL_NAME` to a valid id for the chosen source.\n",
    "  - HF examples: `facebook/dinov3-vitb16-pretrain-lvd1689m`, `google/vit-base-patch16-224`.\n",
    "  - timm examples: `resnet50`, `resnet18`, `efficientnet_b0`, `convnext_tiny`.\n",
    "- Transforms:\n",
    "  - HF: letterbox to `(W,H)` then normalize via the HF image processor.\n",
    "  - timm: deterministic eval transform (Resize+CenterCrop+Normalize) matching the modelâ€™s config.\n",
    "- Everything else (DataBlock, LR finder, training, learning curve, metrics, test eval, saving) stays identical.\n",
    "\n",
    "Notes:\n",
    "- If using timm backbones, ensure `timm` and `torchvision` are installed.\n",
    "- To switch: change `BACKBONE_SRC` and `MODEL_NAME` below; re-run the \\\"Load Backbone\\\" and subsequent cells.\n",
    "\n",
    "Install (fish shell):\n",
    "- `source .venv/bin/activate.fish`\n",
    "- `uv pip install timm torchvision`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0d8e4",
   "metadata": {},
   "source": [
    "> To train a ResNet or EfficientNet model, modify below:\n",
    "\n",
    "+ Set BACKBONE_SRC to \"timm\".\n",
    "+ Set MODEL_NAME to the desired model, for example, \"resnet50\" or \"efficientnet_b0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all ResNet models\n",
    "resnet_models = timm.list_models(\"*resnet*\")\n",
    "print(resnet_models)\n",
    "\n",
    "# Find all pretrained EfficientNet models\n",
    "# effnet_models = timm.list_models('efficientnet*', pretrained=True)\n",
    "# print(effnet_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select backbone source: 'hf' (Transformers) or 'timm' (timm/torchvision)\n",
    "BACKBONE_SRC = \"hf\"\n",
    "MODEL_NAME = \"facebook/dinov3-vitb16-pretrain-lvd1689m\"  # \"facebook/dinov3-vitb16-pretrain-lvd1689m\"\n",
    "BASE_W, BASE_H = 800, 600  # For timm models, this is automatically detected\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 20\n",
    "\n",
    "# Use only a fraction of the dataset for faster experiments\n",
    "DATA_FRACTION = 1  # Only downsamples Training Data\n",
    "SAMPLE_SEED = 2025\n",
    "\n",
    "\n",
    "# Resolve project root robustly (searches upward for a 'data' directory)\n",
    "def _find_project_root(start: Path | None = None) -> Path:\n",
    "    p = Path.cwd() if start is None else Path(start).resolve()\n",
    "    for _ in range(6):\n",
    "        if (p / \".git\").exists() or (p / \"data\").exists():\n",
    "            return p\n",
    "        if p.parent == p:\n",
    "            break\n",
    "        p = p.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _find_project_root()\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\"  # FindingEmo dataset root\n",
    "\n",
    "# Ensure CSV splits exist (train/valid/test) by invoking the split script if needed\n",
    "_train_csv = DATA_ROOT / \"train.csv\"\n",
    "_valid_csv = DATA_ROOT / \"valid.csv\"\n",
    "_test_csv = DATA_ROOT / \"test.csv\"\n",
    "if not (_train_csv.exists() and _valid_csv.exists() and _test_csv.exists()):\n",
    "    import importlib.util as _ilu\n",
    "\n",
    "    split_path = PROJECT_ROOT / \"scripts\" / \"create_train_val_test_splits.py\"\n",
    "    if not split_path.exists():\n",
    "        raise FileNotFoundError(f\"Split script not found: {split_path}\")\n",
    "    spec = _ilu.spec_from_file_location(\"create_splits_mod\", split_path)\n",
    "    mod = _ilu.module_from_spec(spec)\n",
    "    assert spec and spec.loader, \"Unable to load split script\"\n",
    "    spec.loader.exec_module(mod)\n",
    "    mod.create_splits()\n",
    "\n",
    "# Load these paths in subsequent cells\n",
    "CSV_TRAIN = _train_csv  # CSV with columns: image_path,valence,arousal\n",
    "CSV_VALID = _valid_csv  # Validation split\n",
    "CSV_TEST = _test_csv  # Test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0aa9c1",
   "metadata": {},
   "source": [
    "# Load Backbone (HF or timm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified backbone loader: HF (Transformers) or timm/torchvision\n",
    "from types import SimpleNamespace\n",
    "\n",
    "processor = None\n",
    "\n",
    "if BACKBONE_SRC == \"hf\":\n",
    "    from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "    backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    for p in backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "    backbone = backbone.to(device)\n",
    "    backbone.eval()\n",
    "\n",
    "elif BACKBONE_SRC == \"timm\":\n",
    "    import timm\n",
    "    from timm.data import resolve_data_config\n",
    "\n",
    "    tv_backbone = timm.create_model(\n",
    "        MODEL_NAME, pretrained=True, num_classes=0, global_pool=\"avg\"\n",
    "    )\n",
    "    tv_backbone.eval().to(device)\n",
    "\n",
    "    class TimmBackboneWrapper(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, pixel_values):\n",
    "            feats = self.model(pixel_values)\n",
    "            return SimpleNamespace(pooler_output=feats, last_hidden_state=None)\n",
    "\n",
    "    backbone = TimmBackboneWrapper(tv_backbone)\n",
    "\n",
    "    cfg = resolve_data_config({}, model=tv_backbone)\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported BACKBONE_SRC: {BACKBONE_SRC}\")\n",
    "\n",
    "# Determine feature dimension without dummy forward\n",
    "if BACKBONE_SRC == \"hf\":\n",
    "    # Most HF vision backbones expose hidden_size\n",
    "    feat_dim = getattr(getattr(backbone, \"config\", None), \"hidden_size\", None)\n",
    "    if feat_dim is None:\n",
    "        # Fallback: try common attributes\n",
    "        feat_dim = getattr(backbone, \"hidden_size\", None)\n",
    "        if feat_dim is None:\n",
    "            raise RuntimeError(\"Unable to determine feature dim from config\")\n",
    "else:\n",
    "    # timm models typically expose num_features or classifier in_features\n",
    "    tv = getattr(backbone, \"model\", backbone)\n",
    "    feat_dim = getattr(tv, \"num_features\", None)\n",
    "    if feat_dim is None:\n",
    "        try:\n",
    "            clf = tv.get_classifier()\n",
    "            feat_dim = getattr(clf, \"in_features\", None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if feat_dim is None:\n",
    "        raise RuntimeError(\"Unable to determine timm feature dim\")\n",
    "feat_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a6682",
   "metadata": {},
   "source": [
    "## Why Change Scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dda2ae",
   "metadata": {},
   "source": [
    "\n",
    "+ Balanced targets: Valence is âˆ’3..3 and Arousal is 0..6. A simple head works best when both targets live on the same bounded range. Mapping both to [-1,1] makes losses comparable and gradients stable.  \n",
    "\n",
    "+ Match the head: A tanh head naturally outputs [-1,1]. If labels arenâ€™t in that range, the head must learn offsets/scales, which slows learning and can saturate activations.  \n",
    "\n",
    "+ Stable training with tanh: Using [-1,1] targets with a tanh head avoids saturation and keeps gradients stable, while evaluation is still reported back in FE units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a4c05",
   "metadata": {},
   "source": [
    "## How We Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489118e",
   "metadata": {},
   "source": [
    "+ Forward (to train): v_ref = v / 3 maps âˆ’3..3 â†’ âˆ’1..1; a_ref = (a âˆ’ 3) / 3 maps 0..6 â†’ âˆ’1..1.  \n",
    "\n",
    "+ Backward (to report): v = 3Â·v_ref; a = 3Â·a_ref + 3.  \n",
    "\n",
    "+ Head: Adds tanh so predictions are already in [-1,1]. We train with plain MSE on these bipolar targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_to_ref(va: Tensor) -> Tensor:\n",
    "    \"Map FindingEmo Vâˆˆ[-3,3], Aâˆˆ[0,6] -> bipolar space [-1,1]\"\n",
    "    v = va[..., 0] / 3.0\n",
    "    a = (va[..., 1] - 3.0) / 3.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "def ref_to_fe(va_ref: Tensor) -> Tensor:\n",
    "    \"Inverse map: bipolar [-1,1] -> FindingEmo units\"\n",
    "    v = va_ref[..., 0] * 3.0\n",
    "    a = va_ref[..., 1] * 3.0 + 3.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "# CCC utilities are included via torchmetrics; training uses MSE,\n",
    "# and evaluation reports MAE, CCC, and Spearman.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff093705",
   "metadata": {},
   "source": [
    "# Load FindingEmo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d557cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CSV_TRAIN)\n",
    "df_valid = pd.read_csv(CSV_VALID)\n",
    "df_test = pd.read_csv(CSV_TEST)\n",
    "\n",
    "# Optionally downsample only the TRAIN split to a fraction; keep valid/test full\n",
    "if DATA_FRACTION is not None and DATA_FRACTION < 1.0:\n",
    "    df_train = df_train.sample(frac=DATA_FRACTION, random_state=SAMPLE_SEED)\n",
    "\n",
    "# Filter out any rows whose image files are missing on disk (robust to CWD)\n",
    "from pathlib import Path as _Path\n",
    "\n",
    "\n",
    "def _exists_from_root(p):\n",
    "    p = _Path(p)\n",
    "    if not p.is_absolute():\n",
    "        p = PROJECT_ROOT / p\n",
    "    return p.exists()\n",
    "\n",
    "\n",
    "def _filter_existing(df):\n",
    "    m = df[\"image_path\"].apply(_exists_from_root)\n",
    "    dropped = int((~m).sum())\n",
    "    if dropped:\n",
    "        print(f\"Dropped {dropped} rows due to missing files\")\n",
    "    return df.loc[m].reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_train = _filter_existing(df_train)\n",
    "df_valid = _filter_existing(df_valid)\n",
    "df_test = _filter_existing(df_test)\n",
    "\n",
    "# Prune unreadable/corrupt images (keep truncated files loadable)\n",
    "from fastai.vision.utils import verify_images\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _abs_path(p):\n",
    "    p = Path(p)\n",
    "    return p if p.is_absolute() else (PROJECT_ROOT / p)\n",
    "\n",
    "\n",
    "def prune_corrupt_rows(df):\n",
    "    fns = [_abs_path(p) for p in df[\"image_path\"].tolist()]\n",
    "    bad = verify_images(fns)\n",
    "    if len(bad):\n",
    "        bad_abs = {str(p.resolve()) for p in bad}\n",
    "\n",
    "        def keep(p):\n",
    "            q = _abs_path(p).resolve()\n",
    "            return str(q) not in bad_abs\n",
    "\n",
    "        before = len(df)\n",
    "        df = df[df[\"image_path\"].apply(keep)].reset_index(drop=True)\n",
    "        print(f\"Removed {before - len(df)} corrupt images\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = prune_corrupt_rows(df_train)\n",
    "df_valid = prune_corrupt_rows(df_valid)\n",
    "# df_test  = prune_corrupt_rows(df_test)  # optionally prune test\n",
    "\n",
    "# Show a peek\n",
    "len(df_train), len(df_valid), len(df_test), df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4674a7a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0976e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai transform: apply HF processor per item\n",
    "# Aspect-preserving resize to ~800x600 then pad (letterbox)\n",
    "# =======================\n",
    "# =======================\n",
    "# Fastai transform: delegate to HF processor (resize/crop/normalize)\n",
    "# =======================\n",
    "class HFProcessorTransform(Transform):\n",
    "    def __init__(self, processor, target_size=None, normalize=False, rescale=True):\n",
    "        # target_size is (W, H) in pixels or None to use defaults\n",
    "        self.processor = processor\n",
    "        self.target_size = target_size\n",
    "        self.normalize = (\n",
    "            normalize  # keep False for showable images; apply in batch_tfms instead\n",
    "        )\n",
    "        self.rescale = rescale\n",
    "        # cache mean/std for decoding (if provided by the processor)\n",
    "        self._mean = getattr(processor, \"image_mean\", None)\n",
    "        self._std = getattr(processor, \"image_std\", None)\n",
    "\n",
    "    def encodes(self, img: PILImage):\n",
    "        # Ensure we hand a real PIL.Image to the HF processor\n",
    "        if hasattr(img, \"to_image\"):\n",
    "            pil = img.to_image()\n",
    "        elif isinstance(img, Image.Image):\n",
    "            pil = img\n",
    "        else:\n",
    "            import numpy as np\n",
    "\n",
    "            pil = Image.fromarray(np.array(img))\n",
    "        kwargs = dict(return_tensors=\"pt\")\n",
    "        # Ensure values are in [0,1] here and move normalization to batch_tfms\n",
    "        kwargs.update(dict(do_normalize=self.normalize, do_rescale=self.rescale))\n",
    "        if self.target_size is not None:\n",
    "            w, h = self.target_size\n",
    "            # Provide both resize and crop parameters expected by HF processors\n",
    "            kwargs.update(\n",
    "                dict(\n",
    "                    do_resize=True,\n",
    "                    size={\"height\": h, \"width\": w},\n",
    "                    do_center_crop=True,\n",
    "                    crop_size={\"height\": h, \"width\": w},\n",
    "                )\n",
    "            )\n",
    "        try:\n",
    "            proc = self.processor(images=pil, **kwargs)\n",
    "        except TypeError:\n",
    "            proc = self.processor(images=pil, return_tensors=\"pt\")\n",
    "        x = proc.pixel_values[0]\n",
    "        return TensorImage(x)\n",
    "\n",
    "    def decodes(self, x: TensorImage):\n",
    "        import torch\n",
    "\n",
    "        # Unnormalize using processor stats for display\n",
    "        t = x.detach().cpu().float()\n",
    "        if self.normalize and self._mean is not None and self._std is not None:\n",
    "            mean = torch.tensor(self._mean, dtype=t.dtype).view(-1, 1, 1)\n",
    "            std = torch.tensor(self._std, dtype=t.dtype).view(-1, 1, 1)\n",
    "            if t.shape[0] == mean.shape[0]:\n",
    "                t = t * std + mean\n",
    "        t = t.clamp(0, 1)\n",
    "        return PILImage.create(t)\n",
    "\n",
    "\n",
    "# Label getter that reads FE units and maps to bipolar space [-1,1]\n",
    "def get_y_ref(row):\n",
    "    va = torch.tensor([row[\"valence\"], row[\"arousal\"]], dtype=torch.float32)\n",
    "    return fe_to_ref(va)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4317c5b",
   "metadata": {},
   "source": [
    "## DataBlock & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128834f3",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e8bea",
   "metadata": {},
   "source": [
    "Compute target dims: HFâ†’snap to ViT patch; timmâ†’use model's input size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BACKBONE_SRC == \"hf\":\n",
    "    patch = getattr(getattr(backbone, \"config\", None), \"patch_size\", None)\n",
    "    if patch is None:\n",
    "        patch = getattr(\n",
    "            getattr(getattr(backbone, \"config\", None), \"patches\", None), \"size\", None\n",
    "        )\n",
    "    if isinstance(patch, (list, tuple)) and len(patch) > 0:\n",
    "        patch = patch[0]\n",
    "    if patch is None:\n",
    "        patch = 16\n",
    "    W = int(round(BASE_W / patch) * patch)\n",
    "    H = int(round(BASE_H / patch) * patch)\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        from timm.data import resolve_data_config\n",
    "\n",
    "        cfg = resolve_data_config({}, model=getattr(backbone, \"model\", backbone))\n",
    "        H = int(cfg.get(\"input_size\", (3, 224, 224))[1])\n",
    "        W = int(cfg.get(\"input_size\", (3, 224, 224))[2])\n",
    "    except Exception:\n",
    "        H = W = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0298d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dls(df_train, df_valid, bs=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    item_tfms = None\n",
    "    batch_tfms = None\n",
    "\n",
    "    if BACKBONE_SRC == \"hf\":\n",
    "        # Use fastai's built-in Resize, which works well with show_batch\n",
    "        item_tfms = Resize((H, W), method=\"squish\")\n",
    "\n",
    "        # Get normalization stats from the processor and apply as a batch transform\n",
    "        _mean = getattr(processor, \"image_mean\", [0.485, 0.456, 0.406])\n",
    "        _std = getattr(processor, \"image_std\", [0.229, 0.224, 0.225])\n",
    "\n",
    "        # Add standard augmentations for training and normalization for both train/valid\n",
    "        aug_tfms = aug_transforms(\n",
    "            do_flip=True,\n",
    "            flip_vert=False,\n",
    "            max_rotate=10.0,\n",
    "            max_zoom=1.1,\n",
    "            max_lighting=0.2,\n",
    "            max_warp=0.2,\n",
    "        )\n",
    "        batch_tfms = [*aug_tfms, Normalize.from_stats(_mean, _std)]\n",
    "\n",
    "    else:  # This is the CORRECTED 'timm' path\n",
    "        from timm.data import resolve_data_config\n",
    "\n",
    "        # 1. Get the timm model config\n",
    "        cfg = resolve_data_config({}, model=getattr(backbone, \"model\", backbone))\n",
    "\n",
    "        # 2. Store the ORIGINAL mean and std for fastai's Normalize transform\n",
    "        _mean, _std = cfg.get(\"mean\"), cfg.get(\"std\")\n",
    "\n",
    "        # Use pure fastai transforms: center-crop to target size\n",
    "        item_tfms = Resize((H, W), method=\"crop\")\n",
    "\n",
    "        # We add augmentations and fastai's own Normalize to the batch transforms,\n",
    "        # using the ORIGINAL _mean and _std we saved earlier.\n",
    "        aug_tfms = aug_transforms(do_flip=True, max_rotate=10.0, max_zoom=1.1)\n",
    "        batch_tfms = [*aug_tfms, Normalize.from_stats(_mean, _std)]\n",
    "\n",
    "    dblock = DataBlock(\n",
    "        blocks=(ImageBlock, RegressionBlock(n_out=2)),\n",
    "        get_x=ColReader(\"image_path\", pref=PROJECT_ROOT),\n",
    "        get_y=get_y_ref,\n",
    "        item_tfms=item_tfms,\n",
    "        batch_tfms=batch_tfms,\n",
    "    )\n",
    "\n",
    "    dls = dblock.dataloaders(\n",
    "        df_train,\n",
    "        valid_df=df_valid,\n",
    "        bs=bs,\n",
    "        num_workers=num_workers,\n",
    "        path=PROJECT_ROOT,\n",
    "    )\n",
    "\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = df_to_dls(df_train, df_valid)\n",
    "dls.one_batch()[0].shape, dls.one_batch()[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6758c13",
   "metadata": {},
   "source": [
    "Verify input batch tensor shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab29b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = dls.one_batch()\n",
    "print(\"Computed target (W,H):\", (W, H))\n",
    "print(\"Input tensor shape:\", tuple(xb.shape))\n",
    "assert xb.shape[-2:] == (H, W), f\"Shape mismatch: {xb.shape[-2:]} vs {(H, W)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "show_batch_hdr",
   "metadata": {},
   "source": [
    "## Visual Check: Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679630b0",
   "metadata": {},
   "source": [
    "Visually verify labels and transforms (train vs valid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show_batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=9, nrows=3, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.show_batch(max_n=9, nrows=3, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd2e57",
   "metadata": {},
   "source": [
    "# Model: Frozen Backbone + tiny MLP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackboneRegressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        feat_dim: int,\n",
    "        hidden: int | None = None,\n",
    "        p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if hidden and hidden > 0:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim),\n",
    "                nn.Dropout(p),\n",
    "                nn.Linear(feat_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden, 2),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "        else:\n",
    "            # Simple head: Linear -> Tanh (outputs in [-1, 1])\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(feat_dim, 2),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "        # Ensure backbone is frozen\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: pixel_values [B,3,H,W] already processor-normalized\n",
    "        out = self.backbone(pixel_values=x)\n",
    "        if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "            feats = out.pooler_output\n",
    "        else:\n",
    "            # CLS token (ViT) or spatial mean (ConvNeXt-like outputs)\n",
    "            if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "                feats = out.last_hidden_state[:, 0, :]\n",
    "            else:\n",
    "                # [B,C,H,W] -> global avg pool\n",
    "                feats = out.last_hidden_state.mean(dim=(-1, -2))\n",
    "        return self.head(feats)\n",
    "\n",
    "\n",
    "model = BackboneRegressor(backbone, feat_dim, hidden=512, p=0.0).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai metrics & loss\n",
    "# =======================\n",
    "def ccc_v(inp, targ):\n",
    "    # Concordance Corr. Coefficient for Valence in FE units\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return ccc_fn(pred_fe[:, 0], targ_fe[:, 0])\n",
    "\n",
    "\n",
    "def ccc_a(inp, targ):\n",
    "    # Concordance Corr. Coefficient for Arousal in FE units\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return ccc_fn(pred_fe[:, 1], targ_fe[:, 1])\n",
    "\n",
    "\n",
    "def ccc_avg(inp, targ):\n",
    "    return (ccc_v(inp, targ) + ccc_a(inp, targ)) / 2\n",
    "\n",
    "\n",
    "def mae_fe(inp, targ):\n",
    "    # Report MAE in FE units (inverse-transformed)\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return torch.mean(torch.abs(pred_fe - targ_fe))\n",
    "\n",
    "\n",
    "def mae_v_fe(inp, targ):\n",
    "    # Valence MAE in FE units\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return torch.mean(torch.abs(pred_fe[:, 0] - targ_fe[:, 0]))\n",
    "\n",
    "\n",
    "def mae_a_fe(inp, targ):\n",
    "    # Arousal MAE in FE units\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return torch.mean(torch.abs(pred_fe[:, 1] - targ_fe[:, 1]))\n",
    "\n",
    "\n",
    "# Use plain MSE loss in [-1,1] space for training\n",
    "loss_func = nn.MSELoss()\n",
    "# Report CCC and MAE during training/validation; Spearman is computed in the test block\n",
    "metrics = [ccc_v, ccc_a, ccc_avg, mae_v_fe, mae_a_fe, mae_fe]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d097cc8",
   "metadata": {},
   "source": [
    "## Find Optimal LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set log/checkpoint directory and ensure it exists\n",
    "LOG_DIR = PROJECT_ROOT / \"scene\" / \"checkpoints\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "learn = Learner(\n",
    "    dls,\n",
    "    model,\n",
    "    loss_func=loss_func,\n",
    "    metrics=metrics,\n",
    "    path=LOG_DIR,\n",
    "    cbs=[CSVLogger(), EarlyStoppingCallback(patience=5)],\n",
    ").to_fp16()\n",
    "\n",
    "# Choose optimizer and hyperparameters (FastAI will create Adam by default)\n",
    "# You can override like this:\n",
    "learn.opt_func = partial(Adam, wd=1e-2)\n",
    "# Find a good learning rate\n",
    "lr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n",
    "print(\"Suggested LRs:\", lr_min, lr_steep)\n",
    "lr = float(lr_min)\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986d00",
   "metadata": {},
   "source": [
    "## Fit One Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9412b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(EPOCHS, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs targets on the validation set\n",
    "def _show_results_regression(learn, dl=None, max_n=9, nrows=3, figsize=(10, 8)):\n",
    "    \"\"\"A custom function to display regression results for image-to-point tasks.\"\"\"\n",
    "    import torch\n",
    "    from fastai.vision.all import show_images\n",
    "\n",
    "    # Use the validation dataloader if none is provided\n",
    "    dl = dl or learn.dls.valid\n",
    "\n",
    "    # Get one batch of data\n",
    "    xb, yb = dl.one_batch()\n",
    "\n",
    "    # Decode the batch to get human-readable inputs and targets\n",
    "    xb_dec, yb_dec = dl.decode((xb, yb))\n",
    "\n",
    "    # Get model predictions\n",
    "    learn.model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = learn.model(xb.to(learn.dls.device)).cpu()\n",
    "\n",
    "    # Prepare titles for the plots\n",
    "    n = min(max_n, len(xb_dec))\n",
    "    titles = []\n",
    "    for i in range(n):\n",
    "        # Flatten tensors and convert to list for easy formatting\n",
    "        y = yb_dec[i].flatten().tolist()\n",
    "        p = preds[i].flatten().tolist()\n",
    "        titles.append(\n",
    "            f\"Target: ({y[0]:.2f}, {y[1]:.2f})\\nPred:   ({p[0]:.2f}, {p[1]:.2f})\"\n",
    "        )\n",
    "\n",
    "    # Use fastai's show_images to display the results\n",
    "    show_images(xb_dec[:n], titles=titles, nrows=nrows, figsize=figsize)\n",
    "\n",
    "\n",
    "# --- Directly call the working function ---\n",
    "_show_results_regression(learn, max_n=9, figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_loss_curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation loss curves\n",
    "learn.recorder.plot_loss(show_epochs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673028d",
   "metadata": {},
   "source": [
    "Spearmanâ€™s Ï: Only calculated during test evaluation rather than as an epoch metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 3.0  # both V and A scaled by 3 for bipolar space\n",
    "SCALE_SQ = SCALE * SCALE  # 9.0\n",
    "\n",
    "\n",
    "def _resolve_scale() -> float:\n",
    "    \"\"\"Return FEâ†’unit scale; prefer SCALE, else sqrt(SCALE_SQ).\"\"\"\n",
    "    try:\n",
    "        return SCALE  # noqa: F821\n",
    "    except NameError:\n",
    "        return math.sqrt(SCALE_SQ)  # noqa: F821\n",
    "\n",
    "\n",
    "def mae_fe_to_unit(mae_fe):\n",
    "    \"\"\"Convert FE-space MAE to ref-space [-1,1] MAE.\"\"\"\n",
    "    return float(mae_fe) / _resolve_scale()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_fe_to_unit(1.402860)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c535672",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your df_test actually contains the target columns:\n",
    "# Optionally downsample test split for faster evaluation\n",
    "df_test_eval = df_test\n",
    "if DATA_FRACTION is not None and DATA_FRACTION < 1.0:\n",
    "    df_test_eval = df_test.sample(\n",
    "        frac=DATA_FRACTION, random_state=SAMPLE_SEED\n",
    "    ).reset_index(drop=True)\n",
    "test_dl = dls.test_dl(df_test_eval, with_labels=True)\n",
    "test_metrics = learn.validate(dl=test_dl)\n",
    "\n",
    "names = [\n",
    "    \"test_loss\",  # MSE in [-1,1] training space\n",
    "    \"test_ccc_v\",\n",
    "    \"test_ccc_a\",\n",
    "    \"test_ccc_avg\",\n",
    "    \"test_mae_v\",\n",
    "    \"test_mae_a\",\n",
    "    \"test_mae\",  # MAE reported in FE units (see below for per-dim)\n",
    "]\n",
    "safe_metrics = [float(m) if m is not None else 0.0 for m in test_metrics]\n",
    "res_basic = dict(zip(names, safe_metrics))\n",
    "\n",
    "# Full-dataset MAE and Spearman's rho on FE units\n",
    "preds_ref, targs_ref = learn.get_preds(dl=test_dl)\n",
    "preds_fe = ref_to_fe(preds_ref.cpu())\n",
    "targs_fe = ref_to_fe(targs_ref.cpu())\n",
    "\n",
    "# MAE per-dimension and average\n",
    "abs_err = torch.abs(preds_fe - targs_fe)\n",
    "mae_v = abs_err[:, 0].mean().item()\n",
    "mae_a = abs_err[:, 1].mean().item()\n",
    "mae_avg = (mae_v + mae_a) / 2.0\n",
    "\n",
    "from torchmetrics.functional import spearman_corrcoef as tm_spearman\n",
    "\n",
    "rho_v = float(tm_spearman(preds_fe[:, 0].flatten(), targs_fe[:, 0].flatten()).cpu())\n",
    "rho_a = float(tm_spearman(preds_fe[:, 1].flatten(), targs_fe[:, 1].flatten()).cpu())\n",
    "rho_avg = (rho_v + rho_a) / 2.0\n",
    "\n",
    "{\n",
    "    **res_basic,\n",
    "    \"test_mae_v\": mae_v,\n",
    "    \"test_mae_a\": mae_a,\n",
    "    \"test_mae_avg\": mae_avg,\n",
    "    \"test_spearman_v\": rho_v,\n",
    "    \"test_spearman_a\": rho_a,\n",
    "    \"test_spearman_avg\": rho_avg,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea88ab2",
   "metadata": {},
   "source": [
    "**Root cause**: CCC uses Pearson correlation; if predictions or targets have nearâ€‘zero variance, CCCâ€™s denominator goes to zero and torchmetrics returns NaN with a warning.  \n",
    "\n",
    "Above numbers corroborate this: test_loss measured in [-1,1] space scales to FE units by Ã—9 (since vâ†’3v, aâ†’3a). CCC NaN implies the modelâ€™s preds on the test set are (nearly) constant in at least one dim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb701d7",
   "metadata": {},
   "source": [
    "## Save head weights (state_dict) and full model if desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = learn.path\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "head_path = SAVE_DIR / \"dinov3_mlp_head.pth\"\n",
    "torch.save(model.head.state_dict(), head_path)\n",
    "\n",
    "# Optionally export the whole fastai Learner\n",
    "# learn.export(SAVE_DIR/\"learner.pkl\")\n",
    "\n",
    "head_path, head_path.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c304428",
   "metadata": {},
   "source": [
    "# Inference helper (returns V,A in FE units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bd98c",
   "metadata": {},
   "source": [
    "# Export Learner for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Remove file-writing callbacks\n",
    "try:\n",
    "    from fastai.callback.tensorboard import TensorBoardCallback\n",
    "except Exception:\n",
    "    TensorBoardCallback = None\n",
    "\n",
    "to_remove = [CSVLogger] + ([TensorBoardCallback] if TensorBoardCallback else [])\n",
    "learn.remove_cbs([cb for cb in to_remove if cb])\n",
    "\n",
    "# 2) Aggressively null out any lingering file handles on remaining cbs\n",
    "for cb in list(learn.cbs):\n",
    "    for attr in (\"file\", \"f\", \"writer\"):\n",
    "        if hasattr(cb, attr):\n",
    "            try:\n",
    "                getattr(cb, attr).close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            setattr(cb, attr, None)\n",
    "\n",
    "# 3) Export to checkpoints under learn.path\n",
    "SAVE_DIR = learn.path\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "learn.export(SAVE_DIR / \"dinov3_mlp_learner.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0e337",
   "metadata": {},
   "source": [
    "## Load the exported learner and run inference on a single image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_with_export_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner_path = Path(\"./checkpoints/generic_model_learner.pkl\")\n",
    "# learn_inf = load_learner(learner_path)  # loads on CPU by default\n",
    "# # Optionally move to GPU if available in this session\n",
    "# try:\n",
    "#     learn_inf.dls.device = device\n",
    "#     learn_inf.model.to(device)\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "\n",
    "# # Helper to resolve project-root relative image paths\n",
    "# def resolve_from_root(p):\n",
    "#     p = Path(p)\n",
    "#     return p if p.is_absolute() else (PROJECT_ROOT / p)\n",
    "\n",
    "\n",
    "# @torch.inference_mode()\n",
    "# def predict_with_export(img_path: str | Path):\n",
    "#     p = Path(img_path)\n",
    "#     if not p.is_absolute():\n",
    "#         p = resolve_from_root(p)\n",
    "#     if not p.exists():\n",
    "#         raise FileNotFoundError(f\"Image not found: {p}\")\n",
    "#     img = PILImage.create(str(p))\n",
    "#     pred_ref, _, _ = learn_inf.pLOG_DIRredict(img)\n",
    "#     pred_ref_t = torch.as_tensor(pred_ref)\n",
    "#     pred_fe = ref_to_fe(pred_ref_t.unsqueeze(0)).squeeze(0)\n",
    "#     return pred_fe\n",
    "\n",
    "\n",
    "# # Example: randomly sample one existing test image; show GT and prediction\n",
    "# idxs = list(df_test.index)\n",
    "# random.shuffle(idxs)\n",
    "# for idx in idxs:\n",
    "#     row = df_test.loc[idx]\n",
    "#     p_abs = resolve_from_root(row[\"image_path\"])\n",
    "#     if p_abs.exists():\n",
    "#         print(\"Using exported learner on:\", p_abs)\n",
    "#         display(PILImage.create(str(p_abs)))\n",
    "#         print(f\"Ground truth (V,A): ({row['valence']:.3f}, {row['arousal']:.3f})\")\n",
    "#         pred_va = predict_with_export(p_abs)\n",
    "#         print(f\"Predicted (V,A): ({pred_va[0].item():.3f}, {pred_va[1].item():.3f})\")\n",
    "#         break\n",
    "# else:\n",
    "#     raise FileNotFoundError(\"No existing paths found in df_test['image_path'].\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
