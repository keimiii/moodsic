{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef119572",
   "metadata": {},
   "source": [
    "\n",
    "# DINOv3 → Fastai Regression Head (Valence–Arousal)\n",
    "\n",
    "This notebook shows how to **freeze a DINOv3 backbone** (from 🤗 Transformers) and train a **small regression head** in **fastai** for **valence–arousal (V-A)** prediction.\n",
    "\n",
    "- Targets are expected in **FindingEmo** units: **V∈[-3,3]**, **A∈[0,6]**.  \n",
    "- For training we map to a bipolar space: **v_ref = v/3**, **a_ref = (a-3)/3** → both in **[-1,1]**.  \n",
    "- We compute **CCC** (Concordance Corr. Coefficient) per-dimension and the mean CCC.  \n",
    "- Backbone remains **frozen**; only the tiny head trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install latest libraries (uncomment if needed) ---\n",
    "# %pip install -U torch torchvision torchaudio\n",
    "# %pip install -U fastai transformers timm torchmetrics datasets\n",
    "# %pip install -U accelerate\n",
    "#\n",
    "# If you're on Apple Silicon and want MPS acceleration, make sure your PyTorch build supports MPS.\n",
    "# See: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788dbf5",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54646240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, math, random, shutil, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fastai\n",
    "from fastai.vision.all import *\n",
    "from fastai.learner import load_learner\n",
    "from fastai.callback.tracker import CSVLogger\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Metrics\n",
    "from torchmetrics.functional.regression import concordance_corrcoef as ccc_fn\n",
    "\n",
    "# Device (prefers Apple MPS on M-series Macs)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set HuggingFace token for authentication\n",
    "if \"HUGGINGFACE_TOKEN\" in os.environ:\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ac104",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/dinov3-vitb16-pretrain-lvd1689m\"  # you can switch to vits16/vitsplus/vit7b16, etc.\n",
    "IMAGE_SIZE = 608\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 10\n",
    "\n",
    "# Use only a fraction of the dataset for faster experiments\n",
    "DATA_FRACTION = 1  # Only downsamples Training Data\n",
    "SAMPLE_SEED = 2025\n",
    "\n",
    "DATA_ROOT = Path(\"../data\")  # FindingEmo dataset root\n",
    "CSV_TRAIN = Path(\n",
    "    \"../data/train_clean.csv\"\n",
    ")  # CSV with columns: image_path,valence,arousal\n",
    "CSV_VALID = Path(\"../data/valid_clean.csv\")  # Validation split\n",
    "CSV_TEST = Path(\"../data/test_clean.csv\")  # Test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0aa9c1",
   "metadata": {},
   "source": [
    "# Load DINOv3 Processor + Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Freeze backbone\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "backbone = backbone.to(device)\n",
    "backbone.eval()\n",
    "\n",
    "# Infer feature dim for the pooled output\n",
    "with torch.inference_mode():\n",
    "    # Create a single dummy image tensor with processor's expected size\n",
    "    dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE, dtype=torch.float32)\n",
    "    out = backbone(pixel_values=dummy.to(device))\n",
    "    feat_dim = None\n",
    "    if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "        feat_dim = out.pooler_output.shape[-1]\n",
    "    else:\n",
    "        # Fallback: ViT CLS token (last_hidden_state[:, 0, :]) or spatial mean for ConvNext-like\n",
    "        if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "            feat_dim = out.last_hidden_state.shape[-1]\n",
    "        elif hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 4:\n",
    "            feat_dim = out.last_hidden_state.shape[1]\n",
    "        else:\n",
    "            raise RuntimeError(\"Unable to determine DINOv3 feature dimension.\")\n",
    "feat_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a6682",
   "metadata": {},
   "source": [
    "## Why Change Scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dda2ae",
   "metadata": {},
   "source": [
    "\n",
    "+ Balanced targets: Valence is −3..3 and Arousal is 0..6. A simple head works best when both targets live on the same bounded range. Mapping both to [-1,1] makes losses comparable and gradients stable.  \n",
    "\n",
    "+ Match the head: A tanh head naturally outputs [-1,1]. If labels aren’t in that range, the head must learn offsets/scales, which slows learning and can saturate activations.  \n",
    "\n",
    "+ Apples-to-apples with baselines: Align with the baseline’s bipolar [-1,1] training and evaluate back in FE units to avoid misleadingly low metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a4c05",
   "metadata": {},
   "source": [
    "## How We Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489118e",
   "metadata": {},
   "source": [
    "+ Forward (to train): v_ref = v / 3 maps −3..3 → −1..1; a_ref = (a − 3) / 3 maps 0..6 → −1..1.  \n",
    "\n",
    "+ Backward (to report): v = 3·v_ref; a = 3·a_ref + 3.  \n",
    "\n",
    "+ Head: Adds tanh so predictions are already in [-1,1]. We train with plain MSE on these bipolar targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_to_ref(va: Tensor) -> Tensor:\n",
    "    \"Map FindingEmo V∈[-3,3], A∈[0,6] -> bipolar space [-1,1]\"\n",
    "    v = va[..., 0] / 3.0\n",
    "    a = (va[..., 1] - 3.0) / 3.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "def ref_to_fe(va_ref: Tensor) -> Tensor:\n",
    "    \"Inverse map: bipolar [-1,1] -> FindingEmo units\"\n",
    "    v = va_ref[..., 0] * 3.0\n",
    "    a = va_ref[..., 1] * 3.0 + 3.0\n",
    "    return torch.stack([v, a], dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff093705",
   "metadata": {},
   "source": [
    "# Load FindingEmo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d557cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(CSV_TRAIN)\n",
    "df_valid = pd.read_csv(CSV_VALID)\n",
    "df_test = pd.read_csv(CSV_TEST)\n",
    "\n",
    "# Optionally downsample only the TRAIN split to a fraction; keep valid/test full\n",
    "if DATA_FRACTION is not None and DATA_FRACTION < 1.0:\n",
    "    df_train = df_train.sample(frac=DATA_FRACTION, random_state=SAMPLE_SEED)\n",
    "\n",
    "# Show a peek\n",
    "len(df_train), len(df_valid), len(df_test), df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4674a7a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0976e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai transform: apply HF processor per item\n",
    "# Aspect-preserving resize to ~800x600 then pad (letterbox)\n",
    "# =======================\n",
    "class HFProcessorTransform(Transform):\n",
    "    def __init__(self, processor, target_size=(800, 608), pad_color=0):\n",
    "        # target_size is (W, H) in pixels\n",
    "        self.processor = processor\n",
    "        self.target_size = target_size\n",
    "        self.pad_color = pad_color\n",
    "\n",
    "    def encodes(self, img: PILImage):\n",
    "        # Ensure PIL.Image\n",
    "        pil = img if isinstance(img, Image.Image) else PILImage.create(img)\n",
    "        target_w, target_h = self.target_size\n",
    "        w, h = pil.size\n",
    "        # Scale to fit within target while preserving aspect ratio\n",
    "        scale = min(target_w / max(1, w), target_h / max(1, h))\n",
    "        new_w = max(1, int(round(w * scale)))\n",
    "        new_h = max(1, int(round(h * scale)))\n",
    "        resized = pil.resize((new_w, new_h), resample=Image.Resampling.BILINEAR)\n",
    "        # Letterbox pad to target size, centered\n",
    "        canvas = Image.new(\"RGB\", (target_w, target_h), color=self.pad_color)\n",
    "        pad_left = (target_w - new_w) // 2\n",
    "        pad_top = (target_h - new_h) // 2\n",
    "        canvas.paste(resized, (pad_left, pad_top))\n",
    "        # Normalize with HF processor only (disable any internal resize/crop if supported)\n",
    "        try:\n",
    "            proc = self.processor(\n",
    "                images=np.array(canvas),\n",
    "                return_tensors=\"pt\",\n",
    "                do_resize=False,\n",
    "                do_center_crop=False,\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback for processors that don't accept control flags\n",
    "            proc = self.processor(images=np.array(canvas), return_tensors=\"pt\")\n",
    "        x = proc.pixel_values[0]\n",
    "        return TensorImage(x)\n",
    "\n",
    "\n",
    "# Label getter that reads FE units from df and maps to bipolar space [-1,1]\n",
    "def get_y_ref(row):\n",
    "    va = torch.tensor([row[\"valence\"], row[\"arousal\"]], dtype=torch.float32)\n",
    "    return fe_to_ref(va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4317c5b",
   "metadata": {},
   "source": [
    "## Define DataBlock & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bd806",
   "metadata": {},
   "source": [
    "🔹 What are Data Blocks?\n",
    "\n",
    "+ Definition: The Data Block API in fastai is a declarative pipeline for building datasets. It specifies how raw data is transformed into tensors ready for training.\n",
    "\n",
    "+ Purpose: It allows you to define:\n",
    "\n",
    "    + How to get items (e.g., list of image paths, rows from a CSV).\n",
    "\n",
    "    + How to split data (train/valid/test).\n",
    "\n",
    "    + How to label data (e.g., from folder name, CSV column).\n",
    "\n",
    "    + How to apply transforms (augmentations, normalization).\n",
    "\n",
    "Analogy: Think of it as a blueprint for preparing your dataset. Once defined, you can easily create datasets/loaders from different sources with the same structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf46e4d",
   "metadata": {},
   "source": [
    "🔹 What are Data Loaders?\n",
    "\n",
    "+ Definition: A DataLoader is an iterator that efficiently feeds batches of preprocessed data to your model during training and validation.\n",
    "\n",
    "+ Responsibilities:\n",
    "\n",
    "    + Handles batching (mini-batches instead of full dataset).\n",
    "\n",
    "    + Applies transforms on the fly (e.g., data augmentation).\n",
    "\n",
    "    + Manages shuffling, parallel processing (using workers), and device transfer (CPU → GPU).\n",
    "\n",
    "Output: Yields (x_batch, y_batch) tensors to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2ce3f",
   "metadata": {},
   "source": [
    "🔹 Why are they needed?\n",
    "\n",
    "Deep learning training loops need batches of tensors, but:\n",
    "\n",
    "+ Raw data (images, text, CSVs) often lives in heterogeneous formats (files, tables, nested folders).\n",
    "\n",
    "+ Preprocessing needs to be consistent and reproducible (resize, normalize, tokenize).\n",
    "\n",
    "Efficient training requires:\n",
    "\n",
    "+ Mini-batching (to fit in GPU memory).\n",
    "\n",
    "+ Shuffling (to break correlations).\n",
    "\n",
    "+ Parallel loading (so GPU isn’t waiting for CPU).\n",
    "\n",
    "+ Transform pipelines (on-the-fly augmentation).\n",
    "\n",
    "The Data Block + Data Loader system ensures all this happens smoothly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e478d0",
   "metadata": {},
   "source": [
    "🔹 Why not just load data directly into models?\n",
    "\n",
    "If you bypass them and load everything manually, you’d have to:\n",
    "\n",
    "+ Write your own batching logic.\n",
    "\n",
    "+ Handle shuffling & splits.\n",
    "\n",
    "+ Apply augmentations consistently.\n",
    "\n",
    "+ Manage GPU transfers.\n",
    "\n",
    "This is error-prone, repetitive, and inefficient. Models expect tensors of shape (batch_size, features, ...), not arbitrary images/text arrays.  \n",
    "\n",
    "So while it’s technically possible to feed raw NumPy arrays directly, you’d lose:\n",
    "\n",
    "+ Efficiency (no parallelized preloading).\n",
    "\n",
    "+ Flexibility (hard to switch datasets/splits).\n",
    "\n",
    "+ Reusability (need to rewrite data logic every project).\n",
    "\n",
    "+ Integration with training loops, metrics, and fastai callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute target dims near 800x600, snapped to ViT patch size if available\n",
    "BASE_W, BASE_H = 800, 600\n",
    "patch = getattr(getattr(backbone, \"config\", None), \"patch_size\", None)\n",
    "if patch is None:\n",
    "    # Some configs nest it under patches.size (e.g., DeiT)\n",
    "    patch = getattr(\n",
    "        getattr(getattr(backbone, \"config\", None), \"patches\", None), \"size\", None\n",
    "    )\n",
    "if isinstance(patch, (list, tuple)) and len(patch) > 0:\n",
    "    patch = patch[0]\n",
    "if patch is None:\n",
    "    patch = 16  # default to ViT-B/16 (DINOv3)\n",
    "W = int(round(BASE_W / patch) * patch)\n",
    "H = int(round(BASE_H / patch) * patch)\n",
    "\n",
    "\n",
    "def df_to_dls(df_train, df_valid, bs=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    dblock = DataBlock(\n",
    "        blocks=(ImageBlock, RegressionBlock(n_out=2)),\n",
    "        # Prefix paths with project root relative to this notebook (../)\n",
    "        get_x=ColReader(\"image_path\", pref=\"../\"),\n",
    "        get_y=get_y_ref,\n",
    "        item_tfms=[HFProcessorTransform(processor, target_size=(W, H))],\n",
    "    )\n",
    "    dls = dblock.dataloaders(\n",
    "        df_train, valid_df=df_valid, bs=bs, num_workers=num_workers\n",
    "    )\n",
    "    return dls\n",
    "\n",
    "\n",
    "dls = df_to_dls(df_train, df_valid)\n",
    "dls.one_batch()[0].shape, dls.one_batch()[1].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6758c13",
   "metadata": {},
   "source": [
    "Verify input batch tensor shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab29b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = dls.one_batch()\n",
    "print(\"Computed target (W,H):\", (W, H))\n",
    "print(\"Input tensor shape:\", tuple(xb.shape))\n",
    "assert xb.shape[-2:] == (H, W), f\"Shape mismatch: {xb.shape[-2:]} vs {(H, W)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd2e57",
   "metadata": {},
   "source": [
    "# Model: Frozen DINOv3 + tiny MLP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinoV3Regressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        feat_dim: int,\n",
    "        hidden: int | None = None,\n",
    "        p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if hidden and hidden > 0:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim),\n",
    "                nn.Dropout(p),\n",
    "                nn.Linear(feat_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden, 2),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "        else:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim),\n",
    "                nn.Dropout(p),\n",
    "                nn.Linear(feat_dim, 2),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "        # Ensure backbone is frozen\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: pixel_values [B,3,H,W] already processor-normalized\n",
    "        out = self.backbone(pixel_values=x)\n",
    "        if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "            feats = out.pooler_output\n",
    "        else:\n",
    "            # CLS token (ViT) or spatial mean (ConvNeXt-like outputs)\n",
    "            if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "                feats = out.last_hidden_state[:, 0, :]\n",
    "            else:\n",
    "                # [B,C,H,W] -> global avg pool\n",
    "                feats = out.last_hidden_state.mean(dim=(-1, -2))\n",
    "        return self.head(feats)\n",
    "\n",
    "\n",
    "model = DinoV3Regressor(backbone, feat_dim, hidden=512, p=0.1).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai metrics & loss\n",
    "# =======================\n",
    "def ccc_v(inp, targ):\n",
    "    # Compute CCC on FE units by inverse-transforming from [-1,1]\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return ccc_fn(pred_fe[:, 0], targ_fe[:, 0])\n",
    "\n",
    "\n",
    "def ccc_a(inp, targ):\n",
    "    # Compute CCC on FE units by inverse-transforming from [-1,1]\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return ccc_fn(pred_fe[:, 1], targ_fe[:, 1])\n",
    "\n",
    "\n",
    "def ccc_avg(inp, targ):\n",
    "    return (ccc_v(inp, targ) + ccc_a(inp, targ)) / 2\n",
    "\n",
    "\n",
    "def mse_fe(inp, targ):\n",
    "    # Report MSE in FE units (inverse-transformed)\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return F.mse_loss(pred_fe, targ_fe)\n",
    "\n",
    "\n",
    "def mae_fe(inp, targ):\n",
    "    # Report MAE in FE units (inverse-transformed)\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return torch.mean(torch.abs(pred_fe - targ_fe))\n",
    "\n",
    "\n",
    "def mae_v_fe(inp, targ):\n",
    "    # Valence MAE in FE units\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return torch.mean(torch.abs(pred_fe[:, 0] - targ_fe[:, 0]))\n",
    "\n",
    "\n",
    "def mae_a_fe(inp, targ):\n",
    "    # Arousal MAE in FE units\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return torch.mean(torch.abs(pred_fe[:, 1] - targ_fe[:, 1]))\n",
    "\n",
    "\n",
    "# Use plain MSE loss in [-1,1] space for training\n",
    "loss_func = nn.MSELoss()\n",
    "metrics = [ccc_v, ccc_a, ccc_avg, mae_v_fe, mae_a_fe, mae_fe]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d097cc8",
   "metadata": {},
   "source": [
    "## Find Optimal LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls, model, loss_func=loss_func, metrics=metrics, cbs=[CSVLogger()]\n",
    ").to_fp32()\n",
    "\n",
    "# Choose optimizer and hyperparameters (FastAI will create Adam by default)\n",
    "# You can override like this:\n",
    "learn.opt_func = partial(Adam, wd=1e-2)\n",
    "# Find a good learning rate\n",
    "lr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n",
    "print(\"Suggested LRs:\", lr_min, lr_steep)\n",
    "lr = float(lr_min)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lrfind_helper_subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: LR finder with capped iterations for small subsets\n",
    "# lr_min2, lr_steep2 = learn.lr_find(\n",
    "#     suggest_funcs=(minimum, steep), num_it=min(100, len(dls.train))\n",
    "# )\n",
    "# print(\"Subset-safe Suggested LRs:\", lr_min2, lr_steep2)\n",
    "# # Optionally override lr for training below\n",
    "# lr = float(lr_min2)\n",
    "# lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986d00",
   "metadata": {},
   "source": [
    "## Fit One Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9412b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(EPOCHS, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673028d",
   "metadata": {},
   "source": [
    "Spearman’s ρ: Only calculated during test evaluation rather than as an epoch metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 3.0  # both V and A scaled by 3 for bipolar space\n",
    "SCALE_SQ = SCALE * SCALE  # 9.0\n",
    "\n",
    "\n",
    "def mse_fe_to_unit(mse_fe):\n",
    "    \"Convert FE-space MSE (original units) to ref-space [-1,1] MSE.\"\n",
    "    return float(mse_fe) / SCALE_SQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fe_to_unit(3.083589)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c535672",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your df_test actually contains the target columns:\n",
    "test_dl = dls.test_dl(df_test, with_labels=True)\n",
    "test_metrics = learn.validate(dl=test_dl)\n",
    "\n",
    "names = [\n",
    "    \"test_loss\",\n",
    "    \"test_ccc_v\",\n",
    "    \"test_ccc_a\",\n",
    "    \"test_ccc_avg\",\n",
    "    \"test_mae_v\",\n",
    "    \"test_mae_a\",\n",
    "    \"test_mae\",\n",
    "]\n",
    "# Handle None values in metrics\n",
    "safe_metrics = [float(m) if m is not None else 0.0 for m in test_metrics]\n",
    "res_basic = dict(zip(names, safe_metrics))\n",
    "\n",
    "# Full-dataset MAE and Spearman's rho on FE units\n",
    "preds_ref, targs_ref = learn.get_preds(dl=test_dl)\n",
    "preds_fe = ref_to_fe(preds_ref.cpu())\n",
    "targs_fe = ref_to_fe(targs_ref.cpu())\n",
    "\n",
    "# MAE per-dimension and average\n",
    "abs_err = torch.abs(preds_fe - targs_fe)\n",
    "mae_v = abs_err[:, 0].mean().item()\n",
    "mae_a = abs_err[:, 1].mean().item()\n",
    "mae_avg = (mae_v + mae_a) / 2.0\n",
    "\n",
    "\n",
    "def _compute_spearman(x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    x = x.detach().cpu().flatten()\n",
    "    y = y.detach().cpu().flatten()\n",
    "    try:\n",
    "        from torchmetrics.functional import spearman_corrcoef as tm_spearman\n",
    "\n",
    "        val = tm_spearman(x, y)\n",
    "        return float(val.cpu())\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from scipy.stats import spearmanr\n",
    "\n",
    "        r = spearmanr(x.numpy(), y.numpy()).correlation\n",
    "        return float(0.0 if np.isnan(r) else r)\n",
    "    except Exception:\n",
    "        pass\n",
    "    import numpy as np\n",
    "\n",
    "    def rankdata(a: np.ndarray) -> np.ndarray:\n",
    "        sorter = np.argsort(a)\n",
    "        inv = np.empty_like(sorter)\n",
    "        inv[sorter] = np.arange(len(a))\n",
    "        a_sorted = a[sorter]\n",
    "        obs = np.r_[True, a_sorted[1:] != a_sorted[:-1]]\n",
    "        group = obs.cumsum() - 1\n",
    "        counts = np.bincount(group)\n",
    "        cums = np.cumsum(counts)\n",
    "        starts = cums - counts + 1\n",
    "        avgranks = (starts + cums) / 2.0\n",
    "        return avgranks[group][inv]\n",
    "\n",
    "    xn = x.numpy()\n",
    "    yn = y.numpy()\n",
    "    if xn.std() == 0 or yn.std() == 0:\n",
    "        return 0.0\n",
    "    rx = rankdata(xn)\n",
    "    ry = rankdata(yn)\n",
    "    rx = rx - rx.mean()\n",
    "    ry = ry - ry.mean()\n",
    "    denom = np.sqrt((rx**2).sum()) * np.sqrt((ry**2).sum())\n",
    "    return float(0.0 if denom == 0 else (rx @ ry) / denom)\n",
    "\n",
    "\n",
    "rho_v = _compute_spearman(preds_fe[:, 0], targs_fe[:, 0])\n",
    "rho_a = _compute_spearman(preds_fe[:, 1], targs_fe[:, 1])\n",
    "rho_avg = (rho_v + rho_a) / 2.0\n",
    "\n",
    "{\n",
    "    **res_basic,\n",
    "    \"test_mae_v\": mae_v,\n",
    "    \"test_mae_a\": mae_a,\n",
    "    \"test_mae_avg\": mae_avg,\n",
    "    \"test_spearman_v\": rho_v,\n",
    "    \"test_spearman_a\": rho_a,\n",
    "    \"test_spearman_avg\": rho_avg,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea88ab2",
   "metadata": {},
   "source": [
    "**Root cause**: CCC uses Pearson correlation; if predictions or targets have near‑zero variance, CCC’s denominator goes to zero and torchmetrics returns NaN with a warning.  \n",
    "\n",
    "Above numbers corroborate this: test_loss 0.0828 in [-1,1] space corresponds to test_mse ≈ 0.0828×9 = 0.75 in FE units, so scaling is fine; CCC NaN implies the model’s preds on the test set are (nearly) constant in at least one dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a08c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_fe_to_unit(3.1068525314331055)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb701d7",
   "metadata": {},
   "source": [
    "## Save head weights (state_dict) and full model if desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(\"./checkpoints\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "head_path = SAVE_DIR / \"dinov3_mlp_head.pth\"\n",
    "torch.save(model.head.state_dict(), head_path)\n",
    "\n",
    "# Optionally export the whole fastai Learner\n",
    "# learn.export(SAVE_DIR/\"learner.pkl\")\n",
    "\n",
    "head_path, head_path.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c304428",
   "metadata": {},
   "source": [
    "# Inference helper (returns V,A in FE units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15879bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ROOT = (DATA_ROOT.parent).resolve()  # absolute repo root\n",
    "\n",
    "\n",
    "def resolve_from_root(p):\n",
    "    p = Path(p)\n",
    "    return p if p.is_absolute() else (REPO_ROOT / p)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_image(img_path: str | Path):\n",
    "    img_path = Path(img_path)\n",
    "    if not img_path.is_absolute():\n",
    "        img_path = resolve_from_root(img_path)  # resolve exactly once\n",
    "    if not img_path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "    img = PILImage.create(str(img_path))\n",
    "    x = HFProcessorTransform(processor, target_size=(W, H))(img)\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    out_ref = model(x)\n",
    "    out_fe = ref_to_fe(out_ref.cpu())\n",
    "    return out_fe.squeeze(0)\n",
    "\n",
    "\n",
    "# Randomly sample one existing test image; show GT and prediction\n",
    "idxs = list(df_test.index)\n",
    "random.shuffle(idxs)\n",
    "for idx in idxs:\n",
    "    row = df_test.loc[idx]\n",
    "    p_abs = resolve_from_root(row[\"image_path\"])  # becomes absolute\n",
    "    if p_abs.exists():\n",
    "        print(\"Using:\", p_abs)\n",
    "        # Display the image\n",
    "        img_disp = PILImage.create(str(p_abs))\n",
    "        display(img_disp)\n",
    "        # Display ground-truth V/A\n",
    "        print(f\"Ground truth (V,A): ({row['valence']:.3f}, {row['arousal']:.3f})\")\n",
    "        # Predict and display predicted V/A\n",
    "        pred_va = predict_image(p_abs)  # predict_image won’t re-resolve absolute paths\n",
    "        print(f\"Predicted (V,A): ({pred_va[0].item():.3f}, {pred_va[1].item():.3f})\")\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"No existing paths found in df_test['image_path'].\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bd98c",
   "metadata": {},
   "source": [
    "# Export Learner for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_learner_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter callbacks by type without relying on cbs_of_type (compat-friendly)\n",
    "csv_cbs = [cb for cb in getattr(learn, \"cbs\", []) if isinstance(cb, CSVLogger)]\n",
    "if csv_cbs:\n",
    "    # Close and detach any file handles before removing\n",
    "    for cb in csv_cbs:\n",
    "        f = getattr(cb, \"f\", None)\n",
    "        try:\n",
    "            if f is not None and hasattr(f, \"close\") and not f.closed:\n",
    "                f.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            if hasattr(cb, \"f\"):\n",
    "                delattr(cb, \"f\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        learn.remove_cbs(csv_cbs)\n",
    "    except Exception:\n",
    "        try:\n",
    "            learn.cbs = [cb for cb in getattr(learn, \"cbs\", []) if cb not in csv_cbs]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "SAVE_DIR = Path(\"./checkpoints\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "learner_path = SAVE_DIR / \"dinov3_mlp_learner.pkl\"\n",
    "learn.export(learner_path)\n",
    "learner_path, learner_path.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0e337",
   "metadata": {},
   "source": [
    "## Load the exported learner and run inference on a single image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference_with_export_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_path = Path(\"./checkpoints/dinov3_mlp_learner.pkl\")\n",
    "learn_inf = load_learner(learner_path)  # loads on CPU by default\n",
    "# Optionally move to GPU if available in this session\n",
    "try:\n",
    "    learn_inf.dls.device = device\n",
    "    learn_inf.model.to(device)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_with_export(img_path: str | Path):\n",
    "    p = Path(img_path)\n",
    "    if not p.is_absolute():\n",
    "        p = resolve_from_root(p)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {p}\")\n",
    "    img = PILImage.create(str(p))\n",
    "    pred_ref, _, _ = learn_inf.predict(img)\n",
    "    pred_ref_t = torch.as_tensor(pred_ref)\n",
    "    pred_fe = ref_to_fe(pred_ref_t.unsqueeze(0)).squeeze(0)\n",
    "    return pred_fe\n",
    "\n",
    "\n",
    "# Example: randomly sample one existing test image; show GT and prediction\n",
    "idxs = list(df_test.index)\n",
    "random.shuffle(idxs)\n",
    "for idx in idxs:\n",
    "    row = df_test.loc[idx]\n",
    "    p_abs = resolve_from_root(row[\"image_path\"])\n",
    "    if p_abs.exists():\n",
    "        print(\"Using exported learner on:\", p_abs)\n",
    "        display(PILImage.create(str(p_abs)))\n",
    "        print(f\"Ground truth (V,A): ({row['valence']:.3f}, {row['arousal']:.3f})\")\n",
    "        pred_va = predict_with_export(p_abs)\n",
    "        print(f\"Predicted (V,A): ({pred_va[0].item():.3f}, {pred_va[1].item():.3f})\")\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"No existing paths found in df_test['image_path'].\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
