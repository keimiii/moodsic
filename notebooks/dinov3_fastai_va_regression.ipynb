{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef119572",
   "metadata": {},
   "source": [
    "\n",
    "# DINOv3 → Fastai Regression Head (Valence–Arousal)\n",
    "\n",
    "This notebook shows how to **freeze a DINOv3 backbone** (from 🤗 Transformers) and train a **small regression head** in **fastai** for **valence–arousal (V-A)** prediction.\n",
    "\n",
    "- Targets are expected in **FindingEmo** units: **V∈[-3,3]**, **A∈[0,6]**.  \n",
    "- For training we map to a centered reference space: **v_ref = v/3**, **a_ref = (a−3)/3** → both in **[-1,1]**.  \n",
    "- We compute **CCC** (Concordance Corr. Coefficient) per-dimension and the mean CCC.  \n",
    "- Backbone remains **frozen**; only the tiny head trains.\n",
    "\n",
    "> ⚠️ **Placeholders included:** You’ll find sections marked **TODO** where you should plug in your actual FindingEmo dataset (paths/splits). There’s also an optional **synthetic data** generator for a quick smoke test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a117fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install latest libraries (uncomment if needed) ---\n",
    "# %pip install -U torch torchvision torchaudio\n",
    "# %pip install -U fastai transformers timm torchmetrics datasets\n",
    "# %pip install -U accelerate\n",
    "#\n",
    "# If you're on Apple Silicon and want MPS acceleration, make sure your PyTorch build supports MPS.\n",
    "# See: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54646240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, math, random, shutil, json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fastai\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Metrics\n",
    "from torchmetrics.functional.regression import concordance_corrcoef as ccc_fn\n",
    "\n",
    "# Device (prefers Apple MPS on M-series Macs)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0e70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set HuggingFace token for authentication\n",
    "if \"HUGGINGFACE_TOKEN\" in os.environ:\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d801e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "MODEL_NAME = \"facebook/dinov3-vitb16-pretrain-lvd1689m\"  # you can switch to vits16/vitsplus/vit7b16, etc.\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 12\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 3e-3\n",
    "ALPHA_CCC = 0.7  # weight for CCC in the mixed loss: loss = ALPHA_CCC*(1-mean_ccc) + (1-ALPHA_CCC)*MSE\n",
    "\n",
    "# Data placeholders --- edit these for your FindingEmo setup\n",
    "DATA_ROOT = Path(\"../data\")  # FindingEmo dataset root\n",
    "CSV_TRAIN = Path(\n",
    "    \"../data/train_clean_full.csv\"\n",
    ")  # CSV with columns: image_path,valence,arousal\n",
    "CSV_VALID = Path(\"../data/valid_clean_full.csv\")  # Validation split\n",
    "CSV_TEST = Path(\"../data/test_clean_full.csv\")  # Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6616fee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# Load DINOv3 processor + backbone\n",
    "# =======================\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Freeze backbone\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "backbone = backbone.to(device)\n",
    "backbone.eval()\n",
    "\n",
    "# Infer feature dim for the pooled output\n",
    "with torch.inference_mode():\n",
    "    # Create a single dummy image tensor with processor's expected size\n",
    "    dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE, dtype=torch.float32)\n",
    "    out = backbone(pixel_values=dummy.to(device))\n",
    "    feat_dim = None\n",
    "    if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "        feat_dim = out.pooler_output.shape[-1]\n",
    "    else:\n",
    "        # Fallback: ViT CLS token (last_hidden_state[:, 0, :]) or spatial mean for ConvNext-like\n",
    "        if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "            feat_dim = out.last_hidden_state.shape[-1]\n",
    "        elif hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 4:\n",
    "            feat_dim = out.last_hidden_state.shape[1]\n",
    "        else:\n",
    "            raise RuntimeError(\"Unable to determine DINOv3 feature dimension.\")\n",
    "feat_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7383d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Helpers: scaling & CCC\n",
    "# =======================\n",
    "def fe_to_ref(va: Tensor) -> Tensor:\n",
    "    \"Map FindingEmo V∈[-3,3], A∈[0,6] -> reference space [-1,1]\"\n",
    "    v = va[..., 0] / 3.0\n",
    "    a = (va[..., 1] - 3.0) / 3.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "def ref_to_fe(va_ref: Tensor) -> Tensor:\n",
    "    \"Inverse map: reference [-1,1] -> FindingEmo units\"\n",
    "    v = va_ref[..., 0] * 3.0\n",
    "    a = va_ref[..., 1] * 3.0 + 3.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "def ccc_mean(pred: Tensor, targ: Tensor) -> Tensor:\n",
    "    \"Mean CCC across V and A\"\n",
    "    # torchmetrics.functional returns per-output CCC for (N,2) inputs\n",
    "    c = ccc_fn(pred, targ)  # shape: (2,)\n",
    "    c = torch.nan_to_num(c, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return c.mean()\n",
    "\n",
    "\n",
    "class CCCMixedLoss(nn.Module):\n",
    "    \"Mixed loss: alpha*(1-mean CCC) + (1-alpha)*MSE over the two dims\"\n",
    "\n",
    "    def __init__(self, alpha: float = 0.7):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred: Tensor, targ: Tensor) -> Tensor:\n",
    "        mse = F.mse_loss(pred, targ)\n",
    "        ccc = ccc_mean(pred, targ)\n",
    "        return self.alpha * (1.0 - ccc) + (1.0 - self.alpha) * mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d557cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12991,\n",
       " 2808,\n",
       " 2797,\n",
       "                                                       image_path  valence  \\\n",
       " 0  ../data/Run_2/Indignant students hospital/sagada-students.jpg        1   \n",
       " 1   ../data/Run_2/Frustrated toddlers shopping/0015a8db-1600.jpg        2   \n",
       " \n",
       "    arousal  \n",
       " 0        2  \n",
       " 1        5  )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# Load FindingEmo dataset\n",
    "# =======================\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(CSV_TRAIN)\n",
    "df_valid = pd.read_csv(CSV_VALID)\n",
    "df_test = pd.read_csv(CSV_TEST)\n",
    "\n",
    "# Show a peek\n",
    "len(df_train), len(df_valid), len(df_test), df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0976e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai transform: apply HF processor per item\n",
    "# =======================\n",
    "class HFProcessorTransform(Transform):\n",
    "    \"Converts a PILImage to a TensorImage using the HF processor (resize/normalize)\"\n",
    "\n",
    "    def __init__(self, processor, image_size=IMAGE_SIZE):\n",
    "        self.processor = processor\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def encodes(self, img: PILImage):\n",
    "        proc = self.processor(images=np.array(img), return_tensors=\"pt\")\n",
    "        # proc.pixel_values: [1, 3, H, W] (already resized & normalized as model expects)\n",
    "        x = proc.pixel_values[0]\n",
    "        return TensorImage(x)\n",
    "\n",
    "\n",
    "# Label getter that reads FE units from df and maps to reference space [-1,1]\n",
    "def get_y_ref(row):\n",
    "    va = torch.tensor([row[\"valence\"], row[\"arousal\"]], dtype=torch.float32)\n",
    "    return fe_to_ref(va)\n",
    "\n",
    "\n",
    "item_tfms = [HFProcessorTransform(processor, IMAGE_SIZE)]\n",
    "batch_tfms = []  # (you may add augmentations here if desired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200f108a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128, 2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# DataBlock + DataLoaders\n",
    "# =======================\n",
    "def df_to_dls(df_train, df_valid, bs=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    dblock = DataBlock(\n",
    "        blocks=(ImageBlock, RegressionBlock(n_out=2)),\n",
    "        get_x=ColReader(\"image_path\"),\n",
    "        get_y=get_y_ref,\n",
    "        item_tfms=item_tfms,\n",
    "        batch_tfms=batch_tfms,\n",
    "    )\n",
    "    dls = dblock.dataloaders(\n",
    "        df_train, valid_df=df_valid, bs=bs, num_workers=num_workers\n",
    "    )\n",
    "    return dls\n",
    "\n",
    "\n",
    "dls = df_to_dls(df_train, df_valid)\n",
    "dls.one_batch()[0].shape, dls.one_batch()[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c30efe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoV3Regressor(\n",
       "  (backbone): DINOv3ViTModel(\n",
       "    (embeddings): DINOv3ViTEmbeddings(\n",
       "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (rope_embeddings): DINOv3ViTRopePositionEmbedding()\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DINOv3ViTLayer(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): DINOv3ViTAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_scale1): DINOv3ViTLayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): DINOv3ViTMLP(\n",
       "          (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (layer_scale2): DINOv3ViTLayerScale()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# Model: Frozen DINOv3 + tiny MLP head\n",
    "# =======================\n",
    "class DinoV3Regressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        feat_dim: int,\n",
    "        hidden: int | None = None,\n",
    "        p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if hidden and hidden > 0:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim),\n",
    "                nn.Dropout(p),\n",
    "                nn.Linear(feat_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden, 2),\n",
    "            )\n",
    "        else:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim), nn.Dropout(p), nn.Linear(feat_dim, 2)\n",
    "            )\n",
    "        # Ensure backbone is frozen\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: pixel_values [B,3,H,W] already processor-normalized\n",
    "        out = self.backbone(pixel_values=x)\n",
    "        if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "            feats = out.pooler_output\n",
    "        else:\n",
    "            # CLS token (ViT) or spatial mean (ConvNeXt-like outputs)\n",
    "            if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "                feats = out.last_hidden_state[:, 0, :]\n",
    "            else:\n",
    "                # [B,C,H,W] -> global avg pool\n",
    "                feats = out.last_hidden_state.mean(dim=(-1, -2))\n",
    "        return self.head(feats)\n",
    "\n",
    "\n",
    "model = DinoV3Regressor(backbone, feat_dim, hidden=512, p=0.1).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6cf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai metrics & loss\n",
    "# =======================\n",
    "def ccc_v(inp, targ):\n",
    "    return ccc_fn(inp[:, 0], targ[:, 0])\n",
    "\n",
    "\n",
    "def ccc_a(inp, targ):\n",
    "    return ccc_fn(inp[:, 1], targ[:, 1])\n",
    "\n",
    "\n",
    "def ccc_avg(inp, targ):\n",
    "    return (ccc_v(inp, targ) + ccc_a(inp, targ)) / 2\n",
    "\n",
    "\n",
    "loss_func = CCCMixedLoss(alpha=ALPHA_CCC)\n",
    "metrics = [ccc_v, ccc_a, ccc_avg, mse]  # mse here is fastai's MSE metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4736c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested LRs: 0.00020892962347716094 0.005248074419796467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00020892962347716094"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================\n",
    "# Create Learner & train\n",
    "# =======================\n",
    "learn = Learner(\n",
    "    dls, model, loss_func=loss_func, metrics=metrics, cbs=[CSVLogger()]\n",
    ").to_fp32()\n",
    "\n",
    "# Choose optimizer and hyperparameters (FastAI will create Adam by default)\n",
    "# You can override like this:\n",
    "learn.opt_func = partial(Adam, wd=1e-2)\n",
    "# Find a good learning rate\n",
    "lr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n",
    "print(\"Suggested LRs:\", lr_min, lr_steep)\n",
    "lr = float(lr_min)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b9412b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>ccc_v</th>\n",
       "      <th>ccc_a</th>\n",
       "      <th>ccc_avg</th>\n",
       "      <th>mse</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.787101</td>\n",
       "      <td>0.746691</td>\n",
       "      <td>0.094796</td>\n",
       "      <td>0.055444</td>\n",
       "      <td>0.075120</td>\n",
       "      <td>0.330916</td>\n",
       "      <td>04:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.739781</td>\n",
       "      <td>0.696883</td>\n",
       "      <td>0.210624</td>\n",
       "      <td>0.110993</td>\n",
       "      <td>0.160809</td>\n",
       "      <td>0.364830</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716606</td>\n",
       "      <td>0.694742</td>\n",
       "      <td>0.215486</td>\n",
       "      <td>0.120799</td>\n",
       "      <td>0.168142</td>\n",
       "      <td>0.374806</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.701643</td>\n",
       "      <td>0.683138</td>\n",
       "      <td>0.242734</td>\n",
       "      <td>0.109457</td>\n",
       "      <td>0.176096</td>\n",
       "      <td>0.354683</td>\n",
       "      <td>04:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.690012</td>\n",
       "      <td>0.683084</td>\n",
       "      <td>0.246579</td>\n",
       "      <td>0.122519</td>\n",
       "      <td>0.184549</td>\n",
       "      <td>0.374226</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.682162</td>\n",
       "      <td>0.694525</td>\n",
       "      <td>0.225561</td>\n",
       "      <td>0.115075</td>\n",
       "      <td>0.170318</td>\n",
       "      <td>0.379159</td>\n",
       "      <td>10:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.674272</td>\n",
       "      <td>0.678232</td>\n",
       "      <td>0.247853</td>\n",
       "      <td>0.126663</td>\n",
       "      <td>0.187258</td>\n",
       "      <td>0.364376</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.668888</td>\n",
       "      <td>0.683023</td>\n",
       "      <td>0.246041</td>\n",
       "      <td>0.115692</td>\n",
       "      <td>0.180866</td>\n",
       "      <td>0.365432</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.663666</td>\n",
       "      <td>0.679482</td>\n",
       "      <td>0.246397</td>\n",
       "      <td>0.121660</td>\n",
       "      <td>0.184029</td>\n",
       "      <td>0.361008</td>\n",
       "      <td>04:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.656414</td>\n",
       "      <td>0.680002</td>\n",
       "      <td>0.245686</td>\n",
       "      <td>0.120846</td>\n",
       "      <td>0.183266</td>\n",
       "      <td>0.360960</td>\n",
       "      <td>15:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(EPOCHS, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d439c579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.0,\n",
       " 'test_ccc_v': 0.0,\n",
       " 'test_ccc_a': 0.0,\n",
       " 'test_ccc_avg': 0.0,\n",
       " 'test_mse': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# Evaluate on the test set\n",
    "# =======================\n",
    "test_dls = dls.test_dl(df_test)\n",
    "test_metrics = learn.validate(dl=test_dls)\n",
    "names = [\"test_loss\", \"test_ccc_v\", \"test_ccc_a\", \"test_ccc_avg\", \"test_mse\"]\n",
    "# Handle None values in metrics\n",
    "safe_metrics = [float(m) if m is not None else 0.0 for m in test_metrics]\n",
    "dict(zip(names, safe_metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "868f961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('checkpoints/dinov3_head.pth'), True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# Save head weights (state_dict) and full model if desired\n",
    "# =======================\n",
    "SAVE_DIR = Path(\"./checkpoints\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "head_path = SAVE_DIR / \"dinov3_head.pth\"\n",
    "torch.save(model.head.state_dict(), head_path)\n",
    "\n",
    "# Optionally export the whole fastai Learner\n",
    "# learn.export(SAVE_DIR/\"learner.pkl\")\n",
    "\n",
    "head_path, head_path.exists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29634ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorImage([-2.7630,  1.2960])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# Inference helper (returns V,A in FE units)\n",
    "# =======================\n",
    "@torch.inference_mode()\n",
    "def predict_image(img_path: str | Path):\n",
    "    img = PILImage.create(img_path)\n",
    "    x = HFProcessorTransform(processor)(img)  # [3,H,W] tensor\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    out_ref = model(x)  # [-1,1] space\n",
    "    out_fe = ref_to_fe(out_ref.cpu())\n",
    "    return out_fe.squeeze(0)\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "sample_path = df_test.iloc[0][\"image_path\"]\n",
    "predict_image(sample_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353715c8",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & TODOs\n",
    "\n",
    "- **Replace placeholders** (`DATA_ROOT`, `CSV_TRAIN`, `CSV_VALID`, `CSV_TEST`) and set `GENERATE_FAKE_DATA=False` to train on FindingEmo.  \n",
    "- Your CSVs should have at least: `image_path,valence,arousal`.  \n",
    "- We map labels to **[-1,1]** during training; CCC is computed in that same space. If you prefer, invert both preds & labels back to FindingEmo units before computing CCC—the value is invariant if both sides use the same affine map.  \n",
    "- For Apple Silicon, make sure your Python + PyTorch build supports **MPS**. Training will default to CPU if MPS isn’t available.\n",
    "- You can switch backbones by changing `MODEL_NAME` (e.g., `facebook/dinov3-vits16-pretrain-lvd1689m`). Larger models may need lower batch sizes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
