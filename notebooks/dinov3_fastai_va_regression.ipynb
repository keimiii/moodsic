{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef119572",
   "metadata": {},
   "source": [
    "\n",
    "# DINOv3 â†’ Fastai Regression Head (Valenceâ€“Arousal)\n",
    "\n",
    "This notebook shows how to **freeze a DINOv3 backbone** (from ðŸ¤— Transformers) and train a **small regression head** in **fastai** for **valenceâ€“arousal (V-A)** prediction.\n",
    "\n",
    "- Targets are expected in **FindingEmo** units: **Vâˆˆ[-3,3]**, **Aâˆˆ[0,6]**.  \n",
    "- For training we map to a centered reference space: **v_ref = v/3**, **a_ref = (aâˆ’3)/3** â†’ both in **[-1,1]**.  \n",
    "- We compute **CCC** (Concordance Corr. Coefficient) per-dimension and the mean CCC.  \n",
    "- Backbone remains **frozen**; only the tiny head trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a117fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install latest libraries (uncomment if needed) ---\n",
    "# %pip install -U torch torchvision torchaudio\n",
    "# %pip install -U fastai transformers timm torchmetrics datasets\n",
    "# %pip install -U accelerate\n",
    "#\n",
    "# If you're on Apple Silicon and want MPS acceleration, make sure your PyTorch build supports MPS.\n",
    "# See: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788dbf5",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54646240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, math, random, shutil, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fastai\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Metrics\n",
    "from torchmetrics.functional.regression import concordance_corrcoef as ccc_fn\n",
    "\n",
    "# Device (prefers Apple MPS on M-series Macs)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e0e70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set HuggingFace token for authentication\n",
    "if \"HUGGINGFACE_TOKEN\" in os.environ:\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ac104",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/dinov3-vitb16-pretrain-lvd1689m\"  # you can switch to vits16/vitsplus/vit7b16, etc.\n",
    "IMAGE_SIZE = 608\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 12\n",
    "EPOCHS = 2\n",
    "ALPHA_CCC = 0.7  # weight for CCC in the mixed loss: loss = ALPHA_CCC*(1-mean_ccc) + (1-ALPHA_CCC)*MSE\n",
    "\n",
    "# Use only a fraction of the dataset for faster experiments\n",
    "DATA_FRACTION = 0.2  # Only downsamples Training Data\n",
    "SAMPLE_SEED = 2025\n",
    "\n",
    "DATA_ROOT = Path(\"../data\")  # FindingEmo dataset root\n",
    "CSV_TRAIN = Path(\n",
    "    \"../data/train_clean_full.csv\"\n",
    ")  # CSV with columns: image_path,valence,arousal\n",
    "CSV_VALID = Path(\"../data/valid_clean_full.csv\")  # Validation split\n",
    "CSV_TEST = Path(\"../data/test_clean_full.csv\")  # Test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0aa9c1",
   "metadata": {},
   "source": [
    "# Load DINOv3 Processor + Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Freeze backbone\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "backbone = backbone.to(device)\n",
    "backbone.eval()\n",
    "\n",
    "# Infer feature dim for the pooled output\n",
    "with torch.inference_mode():\n",
    "    # Create a single dummy image tensor with processor's expected size\n",
    "    dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE, dtype=torch.float32)\n",
    "    out = backbone(pixel_values=dummy.to(device))\n",
    "    feat_dim = None\n",
    "    if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "        feat_dim = out.pooler_output.shape[-1]\n",
    "    else:\n",
    "        # Fallback: ViT CLS token (last_hidden_state[:, 0, :]) or spatial mean for ConvNext-like\n",
    "        if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "            feat_dim = out.last_hidden_state.shape[-1]\n",
    "        elif hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 4:\n",
    "            feat_dim = out.last_hidden_state.shape[1]\n",
    "        else:\n",
    "            raise RuntimeError(\"Unable to determine DINOv3 feature dimension.\")\n",
    "feat_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7383d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Helpers: scaling & CCC\n",
    "# =======================\n",
    "def fe_to_ref(va: Tensor) -> Tensor:\n",
    "    \"Map FindingEmo Vâˆˆ[-3,3], Aâˆˆ[0,6] -> reference space [-1,1]\"\n",
    "    v = va[..., 0] / 3.0\n",
    "    a = (va[..., 1] - 3.0) / 3.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "def ref_to_fe(va_ref: Tensor) -> Tensor:\n",
    "    \"Inverse map: reference [-1,1] -> FindingEmo units\"\n",
    "    v = va_ref[..., 0] * 3.0\n",
    "    a = va_ref[..., 1] * 3.0 + 3.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "def ccc_mean(pred: Tensor, targ: Tensor) -> Tensor:\n",
    "    \"Mean CCC across V and A\"\n",
    "    # torchmetrics.functional returns per-output CCC for (N,2) inputs\n",
    "    c = ccc_fn(pred, targ)  # shape: (2,)\n",
    "    c = torch.nan_to_num(c, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return c.mean()\n",
    "\n",
    "\n",
    "class CCCMixedLoss(nn.Module):\n",
    "    \"Mixed loss: alpha*(1-mean CCC) + (1-alpha)*MSE over the two dims\"\n",
    "\n",
    "    def __init__(self, alpha: float = 0.7):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred: Tensor, targ: Tensor) -> Tensor:\n",
    "        mse = F.mse_loss(pred, targ)\n",
    "        ccc = ccc_mean(pred, targ)\n",
    "        return self.alpha * (1.0 - ccc) + (1.0 - self.alpha) * mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5d557cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1299,\n",
       " 2808,\n",
       " 2797,\n",
       "                                                                        image_path  \\\n",
       " 7331  ../data/Run_2/Surprised people hugging/e58a56c484418c2f01cd957d7e31e301.jpg   \n",
       " 4491                ../data/Run_2/Repelled thirty-something protest/453583704.jpg   \n",
       " \n",
       "       valence  arousal  \n",
       " 7331        1        2  \n",
       " 4491       -1        4  )"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================\n",
    "# Load FindingEmo dataset\n",
    "# =======================\n",
    "\n",
    "df_train = pd.read_csv(CSV_TRAIN)\n",
    "df_valid = pd.read_csv(CSV_VALID)\n",
    "df_test = pd.read_csv(CSV_TEST)\n",
    "\n",
    "# Optionally downsample only the TRAIN split to a fraction; keep valid/test full\n",
    "if DATA_FRACTION is not None and DATA_FRACTION < 1.0:\n",
    "    df_train = df_train.sample(frac=DATA_FRACTION, random_state=SAMPLE_SEED)\n",
    "\n",
    "# Show a peek\n",
    "len(df_train), len(df_valid), len(df_test), df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0976e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai transform: apply HF processor per item\n",
    "# =======================\n",
    "class HFProcessorTransform(Transform):\n",
    "    def __init__(self, processor, target_size=(800, 608)):\n",
    "        self.processor = processor\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def encodes(self, img: PILImage):\n",
    "        # Resize with padding to target size first\n",
    "        img_resized = img.resize(self.target_size, resample=Image.Resampling.BILINEAR)\n",
    "        # Then apply processor normalization only (without resizing)\n",
    "        proc = self.processor(\n",
    "            images=np.array(img_resized),\n",
    "            size={\"height\": self.target_size[1], \"width\": self.target_size[0]},\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        x = proc.pixel_values[0]\n",
    "        return TensorImage(x)\n",
    "\n",
    "\n",
    "# Label getter that reads FE units from df and maps to reference space [-1,1]\n",
    "def get_y_ref(row):\n",
    "    va = torch.tensor([row[\"valence\"], row[\"arousal\"]], dtype=torch.float32)\n",
    "    return fe_to_ref(va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# DataBlock + DataLoaders\n",
    "# =======================\n",
    "\n",
    "H, W = 800, 608  # -> divisible by 16\n",
    "\n",
    "\n",
    "def df_to_dls(df_train, df_valid, bs=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    dblock = DataBlock(\n",
    "        blocks=(ImageBlock, RegressionBlock(n_out=2)),\n",
    "        get_x=ColReader(\"image_path\"),\n",
    "        get_y=get_y_ref,\n",
    "        item_tfms=[HFProcessorTransform(processor, target_size=(H, W))],\n",
    "    )\n",
    "    dls = dblock.dataloaders(\n",
    "        df_train, valid_df=df_valid, bs=bs, num_workers=num_workers\n",
    "    )\n",
    "    return dls\n",
    "\n",
    "\n",
    "dls = df_to_dls(df_train, df_valid)\n",
    "dls.one_batch()[0].shape, dls.one_batch()[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Model: Frozen DINOv3 + tiny MLP head\n",
    "# =======================\n",
    "class DinoV3Regressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        feat_dim: int,\n",
    "        hidden: int | None = None,\n",
    "        p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if hidden and hidden > 0:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim),\n",
    "                nn.Dropout(p),\n",
    "                nn.Linear(feat_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden, 2),\n",
    "            )\n",
    "        else:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim), nn.Dropout(p), nn.Linear(feat_dim, 2)\n",
    "            )\n",
    "        # Ensure backbone is frozen\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: pixel_values [B,3,H,W] already processor-normalized\n",
    "        out = self.backbone(pixel_values=x)\n",
    "        if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "            feats = out.pooler_output\n",
    "        else:\n",
    "            # CLS token (ViT) or spatial mean (ConvNeXt-like outputs)\n",
    "            if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "                feats = out.last_hidden_state[:, 0, :]\n",
    "            else:\n",
    "                # [B,C,H,W] -> global avg pool\n",
    "                feats = out.last_hidden_state.mean(dim=(-1, -2))\n",
    "        return self.head(feats)\n",
    "\n",
    "\n",
    "model = DinoV3Regressor(backbone, feat_dim, hidden=512, p=0.1).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6cf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai metrics & loss\n",
    "# =======================\n",
    "def ccc_v(inp, targ):\n",
    "    return ccc_fn(inp[:, 0], targ[:, 0])\n",
    "\n",
    "\n",
    "def ccc_a(inp, targ):\n",
    "    return ccc_fn(inp[:, 1], targ[:, 1])\n",
    "\n",
    "\n",
    "def ccc_avg(inp, targ):\n",
    "    return (ccc_v(inp, targ) + ccc_a(inp, targ)) / 2\n",
    "\n",
    "\n",
    "loss_func = CCCMixedLoss(alpha=ALPHA_CCC)\n",
    "metrics = [ccc_v, ccc_a, ccc_avg, mse]  # mse here is fastai's MSE metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Create Learner & train\n",
    "# =======================\n",
    "learn = Learner(\n",
    "    dls, model, loss_func=loss_func, metrics=metrics, cbs=[CSVLogger()]\n",
    ").to_fp32()\n",
    "\n",
    "# Choose optimizer and hyperparameters (FastAI will create Adam by default)\n",
    "# You can override like this:\n",
    "learn.opt_func = partial(Adam, wd=1e-2)\n",
    "# Find a good learning rate\n",
    "lr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n",
    "print(\"Suggested LRs:\", lr_min, lr_steep)\n",
    "lr = float(lr_min)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lrfind_helper_subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: LR finder with capped iterations for small subsets\n",
    "lr_min2, lr_steep2 = learn.lr_find(\n",
    "    suggest_funcs=(minimum, steep), num_it=min(100, len(dls.train))\n",
    ")\n",
    "print(\"Subset-safe Suggested LRs:\", lr_min2, lr_steep2)\n",
    "# Optionally override lr for training below\n",
    "lr = float(lr_min2)\n",
    "lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9412b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(EPOCHS, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c535672",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your df_test actually contains the target columns:\n",
    "test_dl = dls.test_dl(df_test, with_labels=True)\n",
    "test_metrics = learn.validate(dl=test_dl)\n",
    "\n",
    "names = [\"test_loss\", \"test_ccc_v\", \"test_ccc_a\", \"test_ccc_avg\", \"test_mse\"]\n",
    "# Handle None values in metrics\n",
    "safe_metrics = [float(m) if m is not None else 0.0 for m in test_metrics]\n",
    "dict(zip(names, safe_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb701d7",
   "metadata": {},
   "source": [
    "## Save head weights (state_dict) and full model if desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868f961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(\"./checkpoints\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "head_path = SAVE_DIR / \"dinov3_head.pth\"\n",
    "torch.save(model.head.state_dict(), head_path)\n",
    "\n",
    "# Optionally export the whole fastai Learner\n",
    "# learn.export(SAVE_DIR/\"learner.pkl\")\n",
    "\n",
    "head_path, head_path.exists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29634ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Inference helper (returns V,A in FE units)\n",
    "# =======================\n",
    "@torch.inference_mode()\n",
    "def predict_image(img_path: str | Path):\n",
    "    img = PILImage.create(img_path)\n",
    "    x = HFProcessorTransform(processor)(img)  # [3,H,W] tensor\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    out_ref = model(x)  # [-1,1] space\n",
    "    out_fe = ref_to_fe(out_ref.cpu())\n",
    "    return out_fe.squeeze(0)\n",
    "\n",
    "\n",
    "# Quick smoke test\n",
    "sample_path = df_test.iloc[0][\"image_path\"]\n",
    "predict_image(sample_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
