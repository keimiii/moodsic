{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef119572",
   "metadata": {},
   "source": [
    "\n",
    "# DINOv3 â†’ Fastai Regression Head (Valenceâ€“Arousal)\n",
    "\n",
    "This notebook shows how to **freeze a DINOv3 backbone** (from ðŸ¤— Transformers) and train a **small regression head** in **fastai** for **valenceâ€“arousal (V-A)** prediction.\n",
    "\n",
    "- Targets are expected in **FindingEmo** units: **Vâˆˆ[-3,3]**, **Aâˆˆ[0,6]**.  \n",
    "- For training we map to a unit space: **v_unit = (v+3)/6**, **a_unit = a/6** â†’ both in **[0,1]**.  \n",
    "- We compute **CCC** (Concordance Corr. Coefficient) per-dimension and the mean CCC.  \n",
    "- Backbone remains **frozen**; only the tiny head trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a117fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install latest libraries (uncomment if needed) ---\n",
    "# %pip install -U torch torchvision torchaudio\n",
    "# %pip install -U fastai transformers timm torchmetrics datasets\n",
    "# %pip install -U accelerate\n",
    "#\n",
    "# If you're on Apple Silicon and want MPS acceleration, make sure your PyTorch build supports MPS.\n",
    "# See: https://pytorch.org/get-started/locally/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788dbf5",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54646240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, math, random, shutil, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fastai\n",
    "from fastai.vision.all import *\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "# Metrics\n",
    "from torchmetrics.functional.regression import concordance_corrcoef as ccc_fn\n",
    "\n",
    "# Device (prefers Apple MPS on M-series Macs)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0e70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set HuggingFace token for authentication\n",
    "if \"HUGGINGFACE_TOKEN\" in os.environ:\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    login(token=os.environ[\"HUGGINGFACE_TOKEN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ac104",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d801e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/dinov3-vitb16-pretrain-lvd1689m\"  # you can switch to vits16/vitsplus/vit7b16, etc.\n",
    "IMAGE_SIZE = 608\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 8\n",
    "EPOCHS = 2\n",
    "ALPHA_CCC = 0.7  # weight for CCC in the mixed loss: loss = ALPHA_CCC*(1-mean_ccc) + (1-ALPHA_CCC)*MSE\n",
    "\n",
    "# Use only a fraction of the dataset for faster experiments\n",
    "DATA_FRACTION = 0.2  # Only downsamples Training Data\n",
    "SAMPLE_SEED = 2025\n",
    "\n",
    "DATA_ROOT = Path(\"../data\")  # FindingEmo dataset root\n",
    "CSV_TRAIN = Path(\n",
    "    \"../data/train_clean.csv\"\n",
    ")  # CSV with columns: image_path,valence,arousal\n",
    "CSV_VALID = Path(\"../data/valid_clean.csv\")  # Validation split\n",
    "CSV_TEST = Path(\"../data/test_clean.csv\")  # Test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0aa9c1",
   "metadata": {},
   "source": [
    "# Load DINOv3 Processor + Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6616fee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Freeze backbone\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "backbone = backbone.to(device)\n",
    "backbone.eval()\n",
    "\n",
    "# Infer feature dim for the pooled output\n",
    "with torch.inference_mode():\n",
    "    # Create a single dummy image tensor with processor's expected size\n",
    "    dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE, dtype=torch.float32)\n",
    "    out = backbone(pixel_values=dummy.to(device))\n",
    "    feat_dim = None\n",
    "    if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "        feat_dim = out.pooler_output.shape[-1]\n",
    "    else:\n",
    "        # Fallback: ViT CLS token (last_hidden_state[:, 0, :]) or spatial mean for ConvNext-like\n",
    "        if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "            feat_dim = out.last_hidden_state.shape[-1]\n",
    "        elif hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 4:\n",
    "            feat_dim = out.last_hidden_state.shape[1]\n",
    "        else:\n",
    "            raise RuntimeError(\"Unable to determine DINOv3 feature dimension.\")\n",
    "feat_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a6682",
   "metadata": {},
   "source": [
    "## Why Change Scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dda2ae",
   "metadata": {},
   "source": [
    "\n",
    "+ Balanced targets: Valence is âˆ’3..3 and Arousal is 0..6. A simple head works best when both targets live on the same bounded range. Mapping both to [0,1] makes losses comparable and gradients stable.  \n",
    "\n",
    "+ Match the head: A sigmoid head naturally outputs [0,1]. If labels arenâ€™t in that range, the head must learn offsets/scales, which slows learning and can saturate activations.  \n",
    "\n",
    "+ Apples-to-apples with baselines: FindingEmo baselines trained on [0,1] and evaluated back in FE units. Using the same recipe avoids misleadingly low metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208a4c05",
   "metadata": {},
   "source": [
    "## How We Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489118e",
   "metadata": {},
   "source": [
    "+ Forward (to train): v_unit = (v + 3) / 6 maps âˆ’3..3 â†’ 0..1; a_unit = a / 6 maps 0..6 â†’ 0..1.  \n",
    "\n",
    "+ Backward (to report): v = 6Â·v_unit âˆ’ 3; a = 6Â·a_unit.  \n",
    "\n",
    "+ Head: Adds sigmoid so predictions are already in [0,1]. We train with plain MSE on these unit targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7383d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_to_ref(va: Tensor) -> Tensor:\n",
    "    \"Map FindingEmo Vâˆˆ[-3,3], Aâˆˆ[0,6] -> unit space [0,1]\"\n",
    "    v = (va[..., 0] + 3.0) / 6.0\n",
    "    a = va[..., 1] / 6.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "def ref_to_fe(va_ref: Tensor) -> Tensor:\n",
    "    \"Inverse map: unit [0,1] -> FindingEmo units\"\n",
    "    v = va_ref[..., 0] * 6.0 - 3.0\n",
    "    a = va_ref[..., 1] * 6.0\n",
    "    return torch.stack([v, a], dim=-1)\n",
    "\n",
    "\n",
    "def ccc_mean(pred: Tensor, targ: Tensor) -> Tensor:\n",
    "    \"Mean CCC across V and A\"\n",
    "    # torchmetrics.functional returns per-output CCC for (N,2) inputs\n",
    "    c = ccc_fn(pred, targ)  # shape: (2,)\n",
    "    c = torch.nan_to_num(c, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return c.mean()\n",
    "\n",
    "\n",
    "class CCCMixedLoss(nn.Module):\n",
    "    \"Mixed loss: alpha*(1-mean CCC) + (1-alpha)*MSE over the two dims\"\n",
    "\n",
    "    def __init__(self, alpha: float = 0.7):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred: Tensor, targ: Tensor) -> Tensor:\n",
    "        mse = F.mse_loss(pred, targ)\n",
    "        ccc = ccc_mean(pred, targ)\n",
    "        return self.alpha * (1.0 - ccc) + (1.0 - self.alpha) * mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff093705",
   "metadata": {},
   "source": [
    "# Load FindingEmo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d557cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2604,\n",
       " 2792,\n",
       " 2785,\n",
       "                                                              image_path  \\\n",
       " 2030   data/Run_2/Fearful students workplace/o-FEMALE-BOSS-facebook.jpg   \n",
       " 12856                     data/Run_2/Grateful babies fighting/tate2.jpg   \n",
       " \n",
       "        valence  arousal  \n",
       " 2030         1        2  \n",
       " 12856       -3        5  )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(CSV_TRAIN)\n",
    "df_valid = pd.read_csv(CSV_VALID)\n",
    "df_test = pd.read_csv(CSV_TEST)\n",
    "\n",
    "# Optionally downsample only the TRAIN split to a fraction; keep valid/test full\n",
    "if DATA_FRACTION is not None and DATA_FRACTION < 1.0:\n",
    "    df_train = df_train.sample(frac=DATA_FRACTION, random_state=SAMPLE_SEED)\n",
    "\n",
    "# Show a peek\n",
    "len(df_train), len(df_valid), len(df_test), df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4674a7a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0976e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai transform: apply HF processor per item\n",
    "# Aspect-preserving resize to ~800x600 then pad (letterbox)\n",
    "# =======================\n",
    "class HFProcessorTransform(Transform):\n",
    "    def __init__(self, processor, target_size=(800, 608), pad_color=0):\n",
    "        # target_size is (W, H) in pixels\n",
    "        self.processor = processor\n",
    "        self.target_size = target_size\n",
    "        self.pad_color = pad_color\n",
    "\n",
    "    def encodes(self, img: PILImage):\n",
    "        # Ensure PIL.Image\n",
    "        pil = img if isinstance(img, Image.Image) else PILImage.create(img)\n",
    "        target_w, target_h = self.target_size\n",
    "        w, h = pil.size\n",
    "        # Scale to fit within target while preserving aspect ratio\n",
    "        scale = min(target_w / max(1, w), target_h / max(1, h))\n",
    "        new_w = max(1, int(round(w * scale)))\n",
    "        new_h = max(1, int(round(h * scale)))\n",
    "        resized = pil.resize((new_w, new_h), resample=Image.Resampling.BILINEAR)\n",
    "        # Letterbox pad to target size, centered\n",
    "        canvas = Image.new(\"RGB\", (target_w, target_h), color=self.pad_color)\n",
    "        pad_left = (target_w - new_w) // 2\n",
    "        pad_top = (target_h - new_h) // 2\n",
    "        canvas.paste(resized, (pad_left, pad_top))\n",
    "        # Normalize with HF processor only (disable any internal resize/crop if supported)\n",
    "        try:\n",
    "            proc = self.processor(\n",
    "                images=np.array(canvas),\n",
    "                return_tensors=\"pt\",\n",
    "                do_resize=False,\n",
    "                do_center_crop=False,\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback for processors that don't accept control flags\n",
    "            proc = self.processor(images=np.array(canvas), return_tensors=\"pt\")\n",
    "        x = proc.pixel_values[0]\n",
    "        return TensorImage(x)\n",
    "\n",
    "\n",
    "# Label getter that reads FE units from df and maps to unit space [0,1]\n",
    "def get_y_ref(row):\n",
    "    va = torch.tensor([row[\"valence\"], row[\"arousal\"]], dtype=torch.float32)\n",
    "    return fe_to_ref(va)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4317c5b",
   "metadata": {},
   "source": [
    "## Define DataBlock & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bd806",
   "metadata": {},
   "source": [
    "ðŸ”¹ What are Data Blocks?\n",
    "\n",
    "+ Definition: The Data Block API in fastai is a declarative pipeline for building datasets. It specifies how raw data is transformed into tensors ready for training.\n",
    "\n",
    "+ Purpose: It allows you to define:\n",
    "\n",
    "    + How to get items (e.g., list of image paths, rows from a CSV).\n",
    "\n",
    "    + How to split data (train/valid/test).\n",
    "\n",
    "    + How to label data (e.g., from folder name, CSV column).\n",
    "\n",
    "    + How to apply transforms (augmentations, normalization).\n",
    "\n",
    "Analogy: Think of it as a blueprint for preparing your dataset. Once defined, you can easily create datasets/loaders from different sources with the same structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf46e4d",
   "metadata": {},
   "source": [
    "ðŸ”¹ What are Data Loaders?\n",
    "\n",
    "+ Definition: A DataLoader is an iterator that efficiently feeds batches of preprocessed data to your model during training and validation.\n",
    "\n",
    "+ Responsibilities:\n",
    "\n",
    "    + Handles batching (mini-batches instead of full dataset).\n",
    "\n",
    "    + Applies transforms on the fly (e.g., data augmentation).\n",
    "\n",
    "    + Manages shuffling, parallel processing (using workers), and device transfer (CPU â†’ GPU).\n",
    "\n",
    "Output: Yields (x_batch, y_batch) tensors to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2ce3f",
   "metadata": {},
   "source": [
    "ðŸ”¹ Why are they needed?\n",
    "\n",
    "Deep learning training loops need batches of tensors, but:\n",
    "\n",
    "+ Raw data (images, text, CSVs) often lives in heterogeneous formats (files, tables, nested folders).\n",
    "\n",
    "+ Preprocessing needs to be consistent and reproducible (resize, normalize, tokenize).\n",
    "\n",
    "Efficient training requires:\n",
    "\n",
    "+ Mini-batching (to fit in GPU memory).\n",
    "\n",
    "+ Shuffling (to break correlations).\n",
    "\n",
    "+ Parallel loading (so GPU isnâ€™t waiting for CPU).\n",
    "\n",
    "+ Transform pipelines (on-the-fly augmentation).\n",
    "\n",
    "The Data Block + Data Loader system ensures all this happens smoothly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e478d0",
   "metadata": {},
   "source": [
    "ðŸ”¹ Why not just load data directly into models?\n",
    "\n",
    "If you bypass them and load everything manually, youâ€™d have to:\n",
    "\n",
    "+ Write your own batching logic.\n",
    "\n",
    "+ Handle shuffling & splits.\n",
    "\n",
    "+ Apply augmentations consistently.\n",
    "\n",
    "+ Manage GPU transfers.\n",
    "\n",
    "This is error-prone, repetitive, and inefficient. Models expect tensors of shape (batch_size, features, ...), not arbitrary images/text arrays.  \n",
    "\n",
    "So while itâ€™s technically possible to feed raw NumPy arrays directly, youâ€™d lose:\n",
    "\n",
    "+ Efficiency (no parallelized preloading).\n",
    "\n",
    "+ Flexibility (hard to switch datasets/splits).\n",
    "\n",
    "+ Reusability (need to rewrite data logic every project).\n",
    "\n",
    "+ Integration with training loops, metrics, and fastai callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200f108a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 608, 800]), torch.Size([32, 2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute target dims near 800x600, snapped to ViT patch size if available\n",
    "BASE_W, BASE_H = 800, 600\n",
    "patch = getattr(getattr(backbone, \"config\", None), \"patch_size\", None)\n",
    "if patch is None:\n",
    "    # Some configs nest it under patches.size (e.g., DeiT)\n",
    "    patch = getattr(\n",
    "        getattr(getattr(backbone, \"config\", None), \"patches\", None), \"size\", None\n",
    "    )\n",
    "if isinstance(patch, (list, tuple)) and len(patch) > 0:\n",
    "    patch = patch[0]\n",
    "if patch is None:\n",
    "    patch = 16  # default to ViT-B/16 (DINOv3)\n",
    "W = int(round(BASE_W / patch) * patch)\n",
    "H = int(round(BASE_H / patch) * patch)\n",
    "\n",
    "\n",
    "def df_to_dls(df_train, df_valid, bs=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    dblock = DataBlock(\n",
    "        blocks=(ImageBlock, RegressionBlock(n_out=2)),\n",
    "        # Prefix paths with project root relative to this notebook (../)\n",
    "        get_x=ColReader(\"image_path\", pref=\"../\"),\n",
    "        get_y=get_y_ref,\n",
    "        item_tfms=[HFProcessorTransform(processor, target_size=(W, H))],\n",
    "    )\n",
    "    dls = dblock.dataloaders(\n",
    "        df_train, valid_df=df_valid, bs=bs, num_workers=num_workers\n",
    "    )\n",
    "    return dls\n",
    "\n",
    "\n",
    "dls = df_to_dls(df_train, df_valid)\n",
    "dls.one_batch()[0].shape, dls.one_batch()[1].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6758c13",
   "metadata": {},
   "source": [
    "Verify input batch tensor shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab29b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed target (W,H): (800, 608)\n",
      "Input tensor shape: (32, 3, 608, 800)\n"
     ]
    }
   ],
   "source": [
    "xb, yb = dls.one_batch()\n",
    "print(\"Computed target (W,H):\", (W, H))\n",
    "print(\"Input tensor shape:\", tuple(xb.shape))\n",
    "assert xb.shape[-2:] == (H, W), f\"Shape mismatch: {xb.shape[-2:]} vs {(H, W)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fd2e57",
   "metadata": {},
   "source": [
    "# Model: Frozen DINOv3 + tiny MLP head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c30efe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoV3Regressor(\n",
       "  (backbone): DINOv3ViTModel(\n",
       "    (embeddings): DINOv3ViTEmbeddings(\n",
       "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (rope_embeddings): DINOv3ViTRopePositionEmbedding()\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DINOv3ViTLayer(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): DINOv3ViTAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_scale1): DINOv3ViTLayerScale()\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): DINOv3ViTMLP(\n",
       "          (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (layer_scale2): DINOv3ViTLayerScale()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DinoV3Regressor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        feat_dim: int,\n",
    "        hidden: int | None = None,\n",
    "        p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        if hidden and hidden > 0:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim),\n",
    "                nn.Dropout(p),\n",
    "                nn.Linear(feat_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden, 2),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        else:\n",
    "            self.head = nn.Sequential(\n",
    "                nn.LayerNorm(feat_dim),\n",
    "                nn.Dropout(p),\n",
    "                nn.Linear(feat_dim, 2),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        # Ensure backbone is frozen\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x: pixel_values [B,3,H,W] already processor-normalized\n",
    "        out = self.backbone(pixel_values=x)\n",
    "        if hasattr(out, \"pooler_output\") and (out.pooler_output is not None):\n",
    "            feats = out.pooler_output\n",
    "        else:\n",
    "            # CLS token (ViT) or spatial mean (ConvNeXt-like outputs)\n",
    "            if hasattr(out, \"last_hidden_state\") and out.last_hidden_state.ndim == 3:\n",
    "                feats = out.last_hidden_state[:, 0, :]\n",
    "            else:\n",
    "                # [B,C,H,W] -> global avg pool\n",
    "                feats = out.last_hidden_state.mean(dim=(-1, -2))\n",
    "        return self.head(feats)\n",
    "\n",
    "\n",
    "model = DinoV3Regressor(backbone, feat_dim, hidden=512, p=0.1).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6cf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Fastai metrics & loss\n",
    "# =======================\n",
    "def ccc_v(inp, targ):\n",
    "    # Compute CCC on FE units by inverse-transforming from [0,1]\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return ccc_fn(pred_fe[:, 0], targ_fe[:, 0])\n",
    "\n",
    "\n",
    "def ccc_a(inp, targ):\n",
    "    # Compute CCC on FE units by inverse-transforming from [0,1]\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return ccc_fn(pred_fe[:, 1], targ_fe[:, 1])\n",
    "\n",
    "\n",
    "def ccc_avg(inp, targ):\n",
    "    return (ccc_v(inp, targ) + ccc_a(inp, targ)) / 2\n",
    "\n",
    "\n",
    "def mse_fe(inp, targ):\n",
    "    # Report MSE in FE units (inverse-transformed)\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return F.mse_loss(pred_fe, targ_fe)\n",
    "\n",
    "\n",
    "def mae_fe(inp, targ):\n",
    "    # Report MAE in FE units (inverse-transformed)\n",
    "    pred_fe = ref_to_fe(inp)\n",
    "    targ_fe = ref_to_fe(targ)\n",
    "    return torch.mean(torch.abs(pred_fe - targ_fe))\n",
    "\n",
    "\n",
    "# Use plain MSE loss in [0,1] space for training\n",
    "loss_func = nn.MSELoss()\n",
    "metrics = [ccc_v, ccc_a, ccc_avg, mse_fe, mae_fe]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d097cc8",
   "metadata": {},
   "source": [
    "## Find Optimal LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4736c147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested LRs: 6.309573450380412e-08 0.0691830962896347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.309573450380412e-08"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAG4CAYAAACts1jfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVCpJREFUeJzt3Ql4VNXdx/F/FpKQFZJAQkJIQPZ9DYIiIgiidUVLrS1ILb5t0UpRK7hA7auCBSkuVMRdq6+orWit4IJsCooQWcO+k5CNJStZSOZ9/ifMmJAEskwy2/fzPPfJzJ2bm3sTyPxyzv+c42WxWCwCAADgQbwdfQEAAABNjQAEAAA8DgEIAAB4HAIQAADwOAQgAADgcQhAAADA4xCAAACAxyEAAQAAj0MAAgAAHocABAAAPI7DA9DChQslISFBAgICZPDgwbJhw4Yaj92xY4eMGzfOHO/l5SULFiyo9riUlBT51a9+JREREdK8eXPp1auXbNy4sRHvAgAAuBJfR37xJUuWyLRp02TRokUm/GigGTNmjOzevVtat25d5fiCggLp0KGD3HbbbfKnP/2p2nOeOnVKLrvsMhkxYoQsW7ZMWrVqJXv37pWWLVvW+rrKysokNTVVQkJCTNACAADOT5c3zc3NlZiYGPH2vkgbj8WBEhMTLVOmTLE9Ly0ttcTExFhmz5590c+Nj4+3/P3vf6+y/6GHHrJcfvnlDbquo0eP6gKxbGxsbGxsbOJ6m76PX4zDWoCKi4tl06ZNMmPGDNs+TWujRo2S9evX1/u8n3zyiWlF0lai1atXS2xsrPzhD3+QyZMn1/g5RUVFZquYINXRo0clNDS03tcCAACaTk5OjsTFxZkenItxWADKysqS0tJSiYqKqrRfn+/atave5z1w4IC8+OKLpmvt4Ycflh9++EH++Mc/ip+fn0ycOLHaz5k9e7Y8/vjjVfZr+CEAAQDgWmpTvuLwImh70/qd/v37y1NPPSX9+vWTu+++27T+aJ1RTbQVKjs727Zpyw8AAHBfDgtAkZGR4uPjI+np6ZX26/Po6Oh6n7dNmzbSvXv3Svu6desmR44cqfFz/P39ba09tPoAAOD+HNYFpl1SAwYMkBUrVshNN91ka73R5/fcc0+9z6sjwHQUWUV79uyR+Ph4sTftwispKbH7edE0mjVrZkI4AMDzOHQYvNbpaF3OwIEDJTEx0QyDz8/Pl0mTJpnXJ0yYYIqYtUbHWjidnJxse6zz/WzevFmCg4OlY8eOZr8Ojx86dKjpAvv5z39u5hVavHix2exFi6TT0tLk9OnTdjsnHKNFixamxZHpDgDAs3jpUDBHXsALL7wgc+fONYGib9++8txzz5k5gdSVV15pJj184403zPNDhw5J+/btq5xj+PDhsmrVKtvzTz/91NT16Pw/erwGrQuNAquuijwsLMzUA1XXHXb8+HETfnSuosDAQN48XZD+s9d5pTIyMkwI0q5TAIBru9j7t1MFIFf7Bmq3l3apafjRmabh2k6cOGFCUOfOnekOAwAPCkBuNwqssVlrfrTlB67P+nOklgsAPAsBqJ7o9nIP/BwBwDMRgAAAgMchAAEAAI9DAHKkslKRg2tFtn1Y/lGfOxEdWaddRHUZ7n/nnXfa5nUCAMBZOXQeII+W/InI8odEclJ/2hcaI3LN0yLdbxBnoPMp6ZB/raivrWeffda2mCwAAM6KFiBHhZ/3J1QOPyrnePl+fd0J6GzddZ0kUMOSzqsDAEB11u7NlFtfXCeLVu8XRyIANTXt5tKWH6muleTcvuXTG6U7TCeWvPfee2Xq1KnSsmVLiYqKkpdfftk2+3ZISIiZUXvZsmXVdoHphJQabj7//HOzvprOwH3NNdeYVqKausDq+jUrfp2Kli5dWimI/eUvfzETZ7722mvSrl07cy1/+MMfzDxNf/vb30xw07mannzySbt/HwEA9Zd0+LRsPHxK9qTliiMRgJra4XVVW34qsYjkpJQf1wjefPNNsxCtLhGiweT3v/+93Hbbbaa7KykpSUaPHi2//vWvzSzJ1dH98+bNk7ffflvWrFljFpl94IEHGvVr1mT//v0mOC1fvlz+7//+T1599VW57rrr5NixY7J69Wp5+umn5dFHH5Xvv/++TucFADSebSnlf1T3alv78orGQABqannp9j2ujvr06WNCQadOncxyIQEBASac6FIhum/mzJlmduStW7dW+/k6YeCiRYvM+m39+/c3C9fqAraN+TVroovnagtQ9+7d5frrr5cRI0aYhXB1TbkuXbqYFib9uHLlyjqdFwDQeLYeyzYfezs4AFEE3dSCo+x7XB317t3b9liXftDlPHr16mXbp11USpeHqG4acZ05+ZJLLrE91zW09Fh7fc260HXitAut4nn0/N7e3pX21fW8AIDGkZ5TKBm5ReLtJdK9DS1AniV+aPloL6mpsNhLJDS2/LhG0KxZs8pfzcur0j5rnY22rtT28y826quuX1MDzPnnrG6pioud17qvpnsBADim9adT6xBp7ufY9RcJQE3N26d8qLtxfgg69/yaOeXHeahWrVpJbm6uKZS22rx5s0OvCQDQcNuOOUf9jyIAOYLO8/Pzt0RC21Tery1Dut9J5gFylMGDB5uutocfftgUOr/77rtmZBgAwLVtS3GO+h9FDZCjaMjpel35aC8teNaaH+328uCWH6vw8HD55z//KQ8++KAZMj9y5Egz7P3uu+929KUBAOpJSxusAahXrOMDkJeFaXuryMnJMRP6ZWdnVykELiwslIMHD0r79u3NaCa4Nn6eANA0Uk+fkaFzvhZfby/Z/vgYCWjm06Tv3+ejCwwAADRdAXRUSKOEn7oiAAEAgCabALG3E3R/KQIQAABodNtScpxmBJgiAAEAgMYvgD43BN4ZRoApAhAAAGhUx06dkVMFJdLMx0u6RP80g78jEYAAAECjsg5/7xodKv6+ji+AVgQgAADQJCPAejpJAbQiAAEAgEa13YlmgLYiAAEAgEYtgN5qXQPMiVqAWArDgUrLSiUpI0kyCzKlVWAr6d+6v/iwFAYAwI0cOVkgOYVnxc/XWzpHOUcBtCIAOchXh7+SORvmSHpBum1fVGCUTE+cLqPiRzXptdx5551y+vRpWbp0aZN+XQCA59T/dGsTakKQs3CeK/Gw8DNt1bRK4UdlFGSY/fo6AAButQJ8rPN0fykCkAO6vbTlxyJV16C17nt6w9PmOHv78MMPpVevXtK8eXOJiIiQUaNGmRXX33zzTfn444/Fy8vLbKtWrTLHHz16VH7+859LixYtzArtN954oxw6dKjSOV955RXp1q2bWUi0a9eu8o9//MP2mh6r53vvvfdk6NCh5piePXvK6tWr7X5vAADntNUJ638UAaiJac3P+S0/54egtII0c5w9HT9+XG6//Xb5zW9+Izt37jQh55ZbbpFZs2aZkHPNNdeYY3TTsFJSUiJjxoyRkJAQWbt2rXz77bcSHBxsjisuLjbnfOedd2TmzJny5JNPmnM+9dRT8thjj5lAVZGGrPvvv19+/PFHGTJkiFx//fVy4sQJu94fAMD5lJVZZIeTLYFhRQ1QE9OCZ3seV1sabM6ePWtCT3x8vNmnrUFKW4SKiookOjradvw///lPKSsrMy082oqjXn/9ddMapOFp9OjRJjw988wz5pyqffv2kpycLC+99JJMnDjRdq577rlHxo0bZx6/+OKLsnz5cnn11Vflz3/+s13vEQDgXA6dyJfcorPi7+stnVoHizMhADUxHe1lz+Nqq0+fPjJy5EgTerRlRwPMrbfeKi1btqz2+C1btsi+fftMC1BFhYWFsn//fsnPzzcf77rrLpk8ebLtdQ1ZYWGVU762+lj5+vrKwIEDTYsRAMAz6n+6x4SKr49zdToRgJqYDnXX0V5a8FxdHZCXeJnX9Th78vHxkS+//FLWrVsnX3zxhTz//PPyyCOPyPfff1/t8Xl5eTJgwADTzXW+Vq1amdfVyy+/LIMHD67ytQAASE4t7/7qERMqzsa54pgH0Hl+dKi7NexUZH3+UOJDjTIfkHZlXXbZZfL444+behw/Pz/56KOPzMfS0spF1/3795e9e/dK69atpWPHjpU2beGJioqSmJgYOXDgQJXXtSusou+++65SC9GmTZtM4TQAwL3tsAUg56r/cZoAtHDhQklISDCjhLQ1YcOGDTUeu2PHDlNPosfrG/qCBQuqHPOXv/zFNqLJuukIJWeh8/zMv3K+tA5sXWm/tvzo/saYB0hberRIeePGjXLkyBH597//LZmZmSaI6Pdy69atsnv3bsnKyjIF0HfccYdERkaakV9aBH3w4EFT+/PHP/5Rjh07Zs6pQWr27Nny3HPPyZ49e2Tbtm2mTmj+/PlVfr4atHbt2iVTpkyRU6dOmWJsAIB7zwC9I/XcGmBOGIAc3gW2ZMkSmTZtmixatMiEHw00WqOib8ba+nC+goIC6dChg9x2223ypz/9qcbz9ujRQ7766qtKtSfOREPOiLgRTTYTdGhoqKxZs8Z8f3NyckwhtBYwjx071tTkaLjRj9q1tXLlSrnyyivN8Q899JApcs7NzZXY2FhTR6TnUr/97W8lMDBQ5s6da0Z6BQUFmRqjqVOnVvrac+bMMdvmzZtNC9Enn3xiwhUAwH0dzy6UUwUl4uvtJZ2jnasAWjk8FWhrgRbRTpo0yTzXIPTf//5XXnvtNZk+vbyrqKJBgwaZTVX3esXAU3FUkzPSsDMouvxeGpu29Ojoq+poTY/WBZ1Pv3/nD2k/3y9/+UuzXexr11RrBABw7wVQO7YOFn9f56sNdWgXmM4no/UgOiGf7YK8vc3z9evXN+jcWr+iNSraWqTdOdrtUxMdAq6tIhU3AADgnvU/Dg9AWm+ixbdaUFuRPk9LS6v3ebUr7Y033jAtHjrvjNavDBs2zHTjVEfrWLSw17rFxcXV+2sDAACpEICcbwSYU3SBNQata7Hq3bu3CURa8/L++++beWvON2PGDFOHZKUtQISghtPiai2CAwB4nmRrAbSTLYHhFAFIC2F1zpj09MpLQ+hze9bv6OzFnTt3NhP7Vcff399sAACg4U7mF0tqdqF53K1N5Ql1nYVDu8B0/hmdbG/FihW2fbr8gj6vOHtwQ+nIJp21uE2bNnY7Jy0b7oGfIwDYn3X4e0JEoIQENBNn5PAuMO160nWjdAh2YmKiGaatyyxYR4VNmDDBDL/WOh1r4bSuN2V9nJKSYoZX60KdOsRaPfDAA2bBTe32Sk1NNWtWaUuTLgbaUM2aNbMNx9c1tODa9OdY8ecKAHD/AminCEDjx483E/LpquJa+Ny3b19TvGwtjNbRWzoyzEoDTb9+/WzP582bZ7bhw4ebuWyUTtSnYUdXHNch3pdffrmZjVgfN5QGKe1Sy8jIMM91HhzrYqFwrZYfDT/6c9SfJ8t3AEAjBKBY5yyAVl4W+gCq0CJoHQ2WnZ1tm/SvIv2WaVg7ffq0Q64P9qPhR+vNCLEAYD9XPbNKDmTmy5u/SZThne27uHdD3r+dqgXIFembpdYT6UzVumwEXJN2e9HyAwD2lV90Vg5m5Tv1EHhFAGoAffPkDRQAgJ/sPJ4j2rcUFeovkcHOO8LaKRZDBQAA7mGHCxRAKwIQAACwm59WgHfe7i9FAAIAAHazPaW8Bag7LUAAAMATFJ8tk70ZuU5fAK0IQAAAwC72pOdKSalFwpo3k7YtnXuyYAIQAACwi+RzBdDd24Q6/fxqBCAAAGAX220rwDt395ciAAEAAI8aAq8IQAAAoMFKyyxmEkRXKIBWBCAAAGCXAuiC4lIJ8vORDq2CxdkRgAAAQINtPHzKfOzXrqX4eDt3AbQiAAEAgAZLOheA+se3FFdAAAIAAA228fBJ83EgAQgAAHiCjJxCOXryjOjUP/3atRBXQAACAAANsulc91eXqBAJCWgmroAABAAA7FIAPTDBNbq/FAEIAADYJQANcJH6H0UAAgAA9VZYUio7UsqXwBgYHy6uggAEAADqbcvR03K2zCKtQ/ydfgX4ighAAACg3jYd+an7y9lXgK+IAAQAAOpt0yHXq/9RBCAAAFAvZWWWSi1AroQABAAA6uVAVr6cLigRf19v6RETJq6EAAQAAOpl07nlL/rEtRA/X9eKFK51tQAAwGlsdNH6H0UAAgAA9WKt/3GVBVArIgABAIA6O5lfLAcy883j/u0IQAAAwAMknVv+4pJWQdIyyE9cDQEIAADUfwFUF1r+oiICEAAAqPcIMFcsgFYEIAAAUCdniktl89HT5vGg9rQAAQAAD7Dx8EkpKbVIm7AASYgIFFfkFAFo4cKFkpCQIAEBATJ48GDZsGFDjcfu2LFDxo0bZ47XRdcWLFhwwXPPmTPHHDd16tRGuHIAADzP+v0nzMchHSJcagFUpwpAS5YskWnTpsmsWbMkKSlJ+vTpI2PGjJGMjIxqjy8oKJAOHTqYYBMdHX3Bc//www/y0ksvSe/evRvp6gEA8DzrrAHokghxVQ4PQPPnz5fJkyfLpEmTpHv37rJo0SIJDAyU1157rdrjBw0aJHPnzpVf/OIX4u/vX+N58/Ly5I477pCXX35ZWra8cIFWUVGR5OTkVNoAAEBVuYUlsi0l2zwmANVTcXGxbNq0SUaNGvXTBXl7m+fr169v0LmnTJki1113XaVz12T27NkSFhZm2+Li4hr0tQEAcFc/HDoppWUWaRceKG1bumb9j8MDUFZWlpSWlkpUVFSl/fo8LS2t3ud97733THeaBpvamDFjhmRnZ9u2o0eP1vtrAwDgCfU/Q1249Uf5ipvR8HLffffJl19+aYqqa0O70i7UnQYAANyn/sfhASgyMlJ8fHwkPT290n59frEC55pol5oWUPfv39+2T1uZ1qxZIy+88IKp99GvCQAA6uZ0QbEkH8+xjQBzZQ7tAvPz85MBAwbIihUrbPvKysrM8yFDhtTrnCNHjpRt27bJ5s2bbdvAgQNNQbQ+JvwAAFA/3x04KRZL+fpfrUNr18virBzeBaZD4CdOnGhCSmJiopnXJz8/34wKUxMmTJDY2FhbPY8WTicnJ9sep6SkmGATHBwsHTt2lJCQEOnZs2elrxEUFCQRERFV9gMAgNr77oC1/idSXJ3DA9D48eMlMzNTZs6caQqf+/btK8uXL7cVRh85csSMDLNKTU2Vfv362Z7PmzfPbMOHD5dVq1Y55B4AAPAE6/ZnuUX9j/KyWLQxCxXpPEA6HF5HhIWGhjr6cgAAcLjM3CIZ9ORX5nHSY1dLeJCfuPL7t8MnQgQAAK7T/dWtTahThp+6IgABAIDaD3938dFfVgQgAABQhwJoAhAAAPAAx7PPyMGsfPH2EknsEC7ugAAEAABqtfxFr9gwCQ1oJu6AAAQAAGoVgC51k+4vRQACAAAXtOnIKfMxMcE9ur8UAQgAANToVH6xHMjMN4/7tWsp7oIABAAAavTj0fLWnw6RQW4x/48VAQgAANQo6fBp87F/vPu0/igCEAAAqNGmw+UtQP3dqPtLEYAAAEC1zpaWyZZj5S1AA2gBAgAAnmBXWq4UFJdKiL+vdGodLO6EAAQAAKr147nh733btRBvnQbajRCAAABAtZKOnHbL+h9FAAIAABcsgHa3+h9FAAIAAFVk5hbJkZMF4uVV3gXmbghAAACgiqRz9T+dW4e4zQKoFRGAAABAjQGof7z7tf4oAhAAAKgiyU0nQLQiAAEAgEqKz5bJ1mPZbrkEhhUBCAAAVJJ8PEeKzpZJi8BmZhFUd0QAAgAANXZ/eekwMDdEAAIAAJVsOuK+8/9YEYAAAEAlP7p5AbQiAAEAAJvj2WckNbtQfLy9pE9cmLgrAhAAALBJOly+/le3NiES6Ocr7ooABAAAbJKPlw9/7xXrnhMgWhGAAACAzZ70PPOxS1SwuDMCEAAAsNmTnms+do4OEXdGAAIAAMaZ4lKzArzqHEUAAgAAHmBfRp5YLCLhQX4SGewv7owABAAAKnd/uXn9jyIAAQAAY09Grkd0fzlNAFq4cKEkJCRIQECADB48WDZs2FDjsTt27JBx48aZ43V9kgULFlQ55sUXX5TevXtLaGio2YYMGSLLli1r5LsAAMC17UkjADWZJUuWyLRp02TWrFmSlJQkffr0kTFjxkhGRka1xxcUFEiHDh1kzpw5Eh0dXe0xbdu2Na9v2rRJNm7cKFdddZXceOONJjwBAIALD4H3hADkZbFouZPjaIvPoEGD5IUXXjDPy8rKJC4uTu69916ZPn36BT9XW4GmTp1qtosJDw+XuXPnyl133VXltaKiIrNZ5eTkmGvIzs42LUgAALi7vKKz0nPW5+bx5plXS4tAP3E1+v4dFhZWq/dvh7YAFRcXm1aaUaNG/XRB3t7m+fr16+3yNUpLS+W9996T/Px80xVWndmzZ5tvmHXT8AMAgCfZe64AunWIv0uGn7pyaADKysoyASUqKqrSfn2elpbWoHNv27ZNgoODxd/fX373u9/JRx99JN27d6/22BkzZpi0aN2OHj3aoK8NAICr2etB3V/KbVc569Kli2zevNkEmg8//FAmTpwoq1evrjYEaUjSDQAAT7X7XAtQJw8YAu/wABQZGSk+Pj6Snp5eab8+r6nAubb8/PykY8eO5vGAAQPkhx9+kGeffVZeeumlBp0XAAB3ngOoi4e0ADm0C0xDioaTFStW2PZpEbQ+r6lep770vBULnQEAQNUusE4eEoAc3gWmQ+C1e2rgwIGSmJho5vXRguVJkyaZ1ydMmCCxsbGmUNlaOJ2cnGx7nJKSYrq6tN7H2uKjNT1jx46Vdu3aSW5urrz77ruyatUq+fzz8up2AADwk+wzJZKWU+gxs0A7RQAaP368ZGZmysyZM03hc9++fWX58uW2wugjR46YkWFWqamp0q9fP9vzefPmmW348OEm5CidQ0iD0/Hjx82oLp0UUcPP1Vdf7YA7BADANUaAxYQFSEhAM/EEDp8HyNXnEQAAwNW98/1heeSj7TK8cyt58zeJ4qpcZh4gAADgPPU/XaI9o/5HEYAAAPBwe6xD4Ft7Rv2PIgABAODh9pwLQJ4yCaIiAAEA4MFO5hdLVl6xR02CqAhAAAB4sD3nWn/iwptLoJ/DB4c3GQIQAAAebI+HzQBtRQACAMCD7bGtAUYAAgAAHmJPmnUVeM+p/1EEIAAAPJTFYpE9GZ43AkwRgAAA8FCZeUVyuqBEvL1ELmlFCxAAAPAAu9PKW3/iI4IkoJmPeBICEAAAHmrn8RzzsVsbz+r+UgQgAAA81M7j5S1A3aI9b+FvAhAAAOLpLUCh4mkIQAAAeKDis2WyP7N8CHy3GAIQAADwAPsy8qSk1CKhAb4SExYgnoYABACAB3d/dW0TKl5eXuJpCEAAAHhwAOrugfU/igAEAIAH2nVuDiBPHAKvCEAAAHjgEhg7PXgEmCIAAQDgYTJzi+REfrFZAsPT1gCzIgABAOBhks+1/rSP9LwlMKwIQAAAeOoM0G08s/tLEYAAAPAwu9I8u/5HEYAAAPAwOz18CLwiAAEA4EEKS0plf2a+edzVQ4fAKwIQAAAetgRGaZlFWgQ2k+hQz1sCw4oABACAB7HN/xPtmUtgWBGAAADwIIwAa0AAOnr0qBw7dsz2fMOGDTJ16lRZvHhxfU4HAACayE8zQIeIJ6tXAPrlL38pK1euNI/T0tLk6quvNiHokUcekb/+9a/2vkYAAGCvJTAYAl//ALR9+3ZJTEw0j99//33p2bOnrFu3Tt555x1544036nNKAADQyNJyCuV0QYn4eHtJx9bB4snqFYBKSkrE39/fPP7qq6/khhtuMI+7du0qx48ft+8VAgAAu9h1rv7nklaeuwRGgwJQjx49ZNGiRbJ27Vr58ssv5ZprrjH7U1NTJSIiwt7XCAAA7LgGWDcP7/6qdwB6+umn5aWXXpIrr7xSbr/9dunTp4/Z/8knn9i6xupi4cKFkpCQIAEBATJ48GBTT1STHTt2yLhx48zxOnxvwYIFVY6ZPXu2DBo0SEJCQqR169Zy0003ye7du+t8XQAAuGcBdKh4unoFIA0+WVlZZnvttdds+++++27TMlQXS5YskWnTpsmsWbMkKSnJhKkxY8ZIRkZGtccXFBRIhw4dZM6cORIdHV3tMatXr5YpU6bId999Z1qotMtu9OjRkp9fPvMlAACeiAD0Ey+LloTX0ZkzZ0wleWBgoHl++PBh+eijj6Rbt24mvNSFtvhoa80LL7xgnpeVlUlcXJzce++9Mn369At+rrYC6fB73S4kMzPTtARpMLriiiuqvF5UVGQ2q5ycHHMN2dnZEhrKPxIAgHssgdF95nIps4hseHiktHbDWaD1/TssLKxW79/1agG68cYb5a233jKPT58+bULMM888Y7qaXnzxxVqfp7i4WDZt2iSjRo366YK8vc3z9evXi73oN0KFh4dX+7p2mek3zLpp+AEAwJ0czMo34SeseTNpFVI+kMmT1SsAaVfVsGHDzOMPP/xQoqKiTCuQhqLnnnuu1ufRLrTS0lLz+RXpc51fyB60RUlbiC677DIzXL86M2bMMCHJuulEjwAAuNsaYEqHv3t58BIYVr71+SStw9ECY/XFF1/ILbfcYlpuLr30UhOEnInWAum8Rd98802Nx+iQfuuwfgAA3DoAtfLs+X8a1ALUsWNHWbp0qWkp+fzzz02BsdLC5brUzERGRoqPj4+kp6dX2q/Paypwrot77rlHPv30UzNrddu2bRt8PgAA3KEFCPUMQDNnzpQHHnjAFCHrsPchQ4bYWoP69etX6/P4+fnJgAEDZMWKFZW6rPS59Zz1oQXaGn60MPvrr7+W9u3b1/tcAAC4AwKQHbrAbr31Vrn88svNrM/WOYDUyJEj5eabb67TuXQI/MSJE2XgwIEmTOm8PjpcfdKkSeb1CRMmSGxsrClUthZOJycn2x6npKTI5s2bJTg42LRMWbu93n33Xfn4449NV521nkgLnJs3b16fWwYAwGWdLS0zRdCKANSAYfAVWVeFb0gXkw6Bnzt3rgkqffv2NYXUOrLMOueQtjRZ1xg7dOhQtS06w4cPl1WrVpnHNRV3vf7663LnnXfadRgdAADOTsPPiHmrJKCZtyQ/fo14e7tnEXRd3r/rFYC0m+qJJ54wQ9/z8sqb1LSl5f777zcrwmtBtCsjAAEA3MmXyeky+a2N0iMmVP77x/JR3O6oLu/f9eoC05Dz6quvmtmYdXi50lFWf/nLX6SwsFCefPLJ+l05AACwO+p/7BSA3nzzTXnllVdsq8Cr3r17m1qdP/zhDwQgAACcCEPgq6pXX9XJkyela9euVfbrPn0NAAA4j30ZueYjLUANDEA68su6dldFuk9bggAAgHPQUt/9mYwAs0sX2N/+9je57rrr5KuvvrLN16Nrd+nEiJ999ll9TgkAABpBWk6h5BWdFV9vL4mPCHL05bh2C5AOOd+zZ4+Z80cXQ9VNl8PYsWOHvP322/a/SgAA0KD6n/iIQPHzde1R2g5vAVIxMTFVip23bNliRoctXrzYHtcGAAAaiBFg1SMKAgDgxghA1SMAAQDgxvYSgKpFAAIAwI3tt80BFOLoS3HdGiAtdL4QLYYGAADO4VR+sZzILzaPL2nNCLB6ByBdX+Nir+vq7QAAwPH2ZZa3/sS2aC6BfvUe9+SW6vTd0NXUAQCAa6AAumbUAAEA4KYIQDUjAAEA4KYIQDUjAAEA4KYIQDUjAAEA4Ibyi85Kyukz5nHHVgSg8xGAAABwQwfOrQAfEeQnLYP8HH05TocABACAG9qXmWs+0v1VPQIQAABuiPqfCyMAAQDghghAF0YAAgDADe1NJwBdCAEIAAA3HAF28ER5EXS3NqGOvhynRAACAMDN7ErLFYtFpHWIv0QG+zv6cpwSAQgAADez83iO+dg9htafmhCAAABwM8nWAET3V40IQAAAuJnkVFqALoYABACAGykts8iutPIARAF0zQhAAAC4kUMn8qWwpEyaN/ORhIggR1+O0yIAAQDght1fXduEiI+3l6Mvx2kRgAAAcMMCaLq/LowABACAOxZAE4AuiAAEAIAbYQ6g2iEAAQDgJjJziyQjt0i8vES6Roc4+nKcmsMD0MKFCyUhIUECAgJk8ODBsmHDhhqP3bFjh4wbN84c7+XlJQsWLKhyzJo1a+T666+XmJgYc8zSpUsb+Q4AAHCu1p/2EUES6Ofr6Mtxag4NQEuWLJFp06bJrFmzJCkpSfr06SNjxoyRjIyMao8vKCiQDh06yJw5cyQ6OrraY/Lz8815NFgBAOCRBdB0f12UQ+Ph/PnzZfLkyTJp0iTzfNGiRfLf//5XXnvtNZk+fXqV4wcNGmQ2Vd3rauzYsWari6KiIrNZ5eSU/wMCAMCVUADtAi1AxcXFsmnTJhk1atRPF+PtbZ6vX7++Sa9l9uzZEhYWZtvi4uKa9OsDAGAPFEC7QADKysqS0tJSiYqKqrRfn6elpTXptcyYMUOys7Nt29GjR5v06wMA0FCFJaWyPzPPPKYF6OKokBIRf39/swEA4Kp2p+VKmUUkIshPWofwnua0LUCRkZHi4+Mj6enplfbr85oKnAEAwIULoLX7S0dBw0kDkJ+fnwwYMEBWrFhh21dWVmaeDxkyxFGXBQCAa9f/0P3l/F1gOgR+4sSJMnDgQElMTDTz+ugwduuosAkTJkhsbKwpUrYWTicnJ9sep6SkyObNmyU4OFg6duxo9ufl5cm+fftsX+PgwYPmmPDwcGnXrp1D7hMAgKYaAcYaYC4QgMaPHy+ZmZkyc+ZMU/jct29fWb58ua0w+siRI2ZkmFVqaqr069fP9nzevHlmGz58uKxatcrs27hxo4wYMaJSyFIatN54440mvDsAAJpGWZmFEWB15GWxWCx1/SR3p/MA6XB4HREWGso/JACAczuUlS9Xzlslfr7ekvz4GPH1cfhCD07//u2Z3yEAANyItfVH1//y1PBTV3yXAABwlyUwoum1qC0CEAAALm5Peq752IUV4GuNAAQAgIs7mJVvPnZoFeToS3EZBCAAAFxYaZlFDp0oMI8vaRXs6MtxGQQgAABcWOrpM1J8tkz8fLwlpkVzR1+OyyAAAQDgwg6c6/6KjwgUH2+WwKgtAhAAAC7s4LkV4Kn/qRsCEAAAblAA3T6S+p+6IAABAOAGXWAdImkBqgsCEAAALuxA5rkWILrA6oQABACAiyosKZXU7DPmMS1AdUMAAgDARR06kS+6pHlogK+EB/k5+nJcCgEIAAAXddDW/RUsXl4Mga8LAhAAAC5eAH0J3V91RgACAMDlh8ATgOqKAAQAgIs6cG4SREaA1R0BCAAAF0ULUP0RgAAAcEGn8ovlVEGJeUwAqjsCEAAALlwA3SYsQAL9fB19OS6HAAQAgAui+6thCEAAALigg1msAt8QBCAAAFwQq8A3DAEIAAAXXgSVNcDqhwAEAICLKSuzUAPUQAQgAABczPGcQik6WybNfLykbcvmjr4cl0QAAgDARRdBbRceKL4+vJXXBxMHNKHtKdny323HTVq/Y3C8oy8HAOCiDpwbAUYBdP0RG5vQ/sw8eXHVfvl0y3FHXwoAwA0KoC9hCHy9EYCaUGSwv/l4Ir/I0ZcCAHBhFEA3HAGoCUUE+5mPJ/KKHX0pAAC36AIjANUXAagJRQSVtwCdLCiW0jKLoy8HAOCCis6WyrFTZ8zj9nSB1RsBqAm1DGwmXl4iFovIyXxagQAAdXfkRIF5Hwnx95VW50orUHcEoCakQxXDA891g1EHBABowCrw2vrjpX9Vw3UD0MKFCyUhIUECAgJk8ODBsmHDhhqP3bFjh4wbN84crz/4BQsWNPicTYk6IABAQ0cUK+p/XDwALVmyRKZNmyazZs2SpKQk6dOnj4wZM0YyMjKqPb6goEA6dOggc+bMkejoaLuc0xF1QFl5tAABAOpuX0Z5AOrYijmAXDoAzZ8/XyZPniyTJk2S7t27y6JFiyQwMFBee+21ao8fNGiQzJ07V37xi1+Iv7+/Xc5ZVFQkOTk5lbbGbgHKogUIAFAP+60BqDUByGUDUHFxsWzatElGjRr10wV5e5vn69evb7Jzzp49W8LCwmxbXFycNPpcQLQAAQDqyGKxyP5zkyASgFw4AGVlZUlpaalERUVV2q/P09LSmuycM2bMkOzsbNt29OhRaSyR1AABAOopPadI8orOio+3l8RHUAPUEKwFJmK60mrqTrO3CGaDBgA0sP4nPiJQ/HwdXsXi0hz63YuMjBQfHx9JT0+vtF+f11Tg7Ihz2lNEEDVAAID62ZeRaz5eQgG0awcgPz8/GTBggKxYscK2r6yszDwfMmSI05yzMVqAGAUGAKirfeeGwFP/4wZdYDpcfeLEiTJw4EBJTEw08/rk5+ebEVxqwoQJEhsbawqVrUXOycnJtscpKSmyefNmCQ4Olo4dO9bqnI5knbWTGiAAQF0xBN6NAtD48eMlMzNTZs6caYqU+/btK8uXL7cVMR85csSM4rJKTU2Vfv362Z7PmzfPbMOHD5dVq1bV6pyOZB0Gf6akVAqKz0qgn8N/BAAAF7EvgxFg9uJl0TF1qETnAdLh8DoiLDQ01K7n1m93t5nLpbCkTNb+eYTEhQfa9fwAAPeUXVAiff76hXm8/fExEuzPH9ANef+mhLyJ6fId1tmgM6kDAgDUsf4nOjSA8GMHBCAHYC4gAEBdMQO0fRGAHIDZoAEAdcUIMPsiADlyRfh8WoAAAHUbAXYJAcguCEAOnAsoM5cWIABA7TAE3r4IQA6cDZoWIABAbRSWlMrRUwXm8SWtWQPMHghADtAqhBogAEDtHczKF520JjTA1zahLhqGAOQA1mHwjAIDANSp+6t1sJlOBQ1HAHJgETTrgQEA6hqAYB8EIAcGoJMFxVJaxkTcAIALYwi8/RGAHCA80E+0BVP7c08V0A0GALgwJkG0PwKQA/j6eEvLQGaDBgBcnPYUHMgqXwT1EobA2w0ByNFD4akDAgBcwLFTBVJ8tkz8fL2lbUsW0LYXApCD64BYEBUAUJsC6A6RQeLjzQgweyEAOXg2aLrAAAAXwgiwxkEAchDrRFYn8mkBAgDUjADUOAhADq8BogUIAFCz/eeGwFMAbV8EIAd3gTEZIgCgJhaLhRagRkIAcvhs0LQAAQCqpwNlcgrPitY+t49kEVR7IgA5SCQ1QACAi0g6fMp8TIgMkoBmPo6+HLdCAHKQyHMtQNQAAQBqsmJnhvl4ZefWjr4Ut0MAcnANUEFxqRQUn3X05QAAnExZmUVW7i4PQCO7EYDsjQDkIEF+PuLvW/7tpxUIAHC+zcdOmzrREH9fGZQQ7ujLcTsEIAfx8vKy1QExEgwAcL6vz3V/XdGllVkGA/bFd9SBqAMCANTkq53p5uMour8aBQHIgZgLCABQ0wKou9JyzfB3CqAbBwHIGWaDzqcFCADwk693lXd/DYhvKS3PvVfAvghADkQLEADgQsPfR3aLcvSluC1fR1+AJ6MGCAAaV2lZqSRlJElmQaa0Cmwl/Vv3Fx9v555QML/orKzff8I8HtmV7q/GQgByIEaBAUDj+erwVzJnwxxJLygvJlZRgVEyPXG6jIofJc5q7d4sKS4tk3bhgaz/1YjoAnOC9cBoAQIA+4efaaumVQo/KqMgw+zX153V17vSbZMf6pQpaBwEIAeKCGI9MACo7azIadmFpsU8r+islJZZLtjtpS0/Fql6jHXf0xueNsc5431+vSvTPB5F/U+jogvMCWqATuYXm//MPjreEQAgGbmFknT4tGw9dlq2HNOP2ZJbWHnZoGY+XmaBUH9fHwlo5m1m19fHxc32SnpQ5Zaf80NQWkGaqQ0aFD1InIneq4Y8Zn9ufAQgBwo/N7RR/5A5XVBsGxUGAJ6qpLRMnluxVxau3Gd+N1akfyNW3FdSapGS0rOSK5WDkW/ocWkedPGvpYXRzjr664rOzP7sEQFo4cKFMnfuXElLS5M+ffrI888/L4mJiTUe/8EHH8hjjz0mhw4dkk6dOsnTTz8t1157re319PR0eeihh+SLL76Q06dPyxVXXGHOqcc6E18fb2kZ2ExOFZSY9V4IQAA82ZETBXLfkh/lxyOnzfOu0SHSN66F9G7bQvrEhUnnqBDx9faSorNlcqa4VM6UlEphSal5rpv18a7TvvLCzvcu+vV0VJizTX74323HzWMWP/WAALRkyRKZNm2aLFq0SAYPHiwLFiyQMWPGyO7du6V166r/ANatWye33367zJ49W372s5/Ju+++KzfddJMkJSVJz549xWKxmOfNmjWTjz/+WEJDQ2X+/PkyatQoSU5OlqCgWvxZ0IQ09GgAOmFGgoU4+nIAwCE+3pwij3y03dT3hAT4ylM395Lr+8RUe6x2e+nWsoZzXV42Sj44HGUKnqutA7KI+HuFS6hXJ/Oe4ehC48zcItPi9e73R8zor2B/X7myCwGosXlZ9KfvQBp6Bg0aJC+88IJ5XlZWJnFxcXLvvffK9OnTqxw/fvx4yc/Pl08//dS279JLL5W+ffuaELVnzx7p0qWLbN++XXr06GE7Z3R0tDz11FPy29/+tso5i4qKzGaVk5NjriE7O9sEqMY0/qX18v3Bk/Lc7f3khhr+swOAPemv/dV7MuXFVftle0q29IlrIcM6tZJhnSKle5tQ8W7EesRT+cVmiQet8dE3/ozcItmdlmuuRw2MbykLftFX2rYMtMsoMHV+CNJ3vcKUX8nZ3J7SIrCZ9G/X0sy4rB/7tWthwlVjFTjnF5+V/KJSySsqkbyiUvkyOU1e++aQac1SQy+JkIev7SY9Y8Ma5RrcXU5OjoSFhdXq/duhLUDFxcWyadMmmTFjhm2ft7e3aa1Zv359tZ+j+7XFqCJtMVq6dKl5bA0yAQEBlc7p7+8v33zzTbUBSFuTHn/8cXGEyJBzI8GYCwhAI9PBFsu2HzfBZ0dqjm3/uv0nzPb08vLaxMs7RpoalCs6RUrr0J9+l9ZXTmGJfLEjXf6zJVW+2ZdV7QguzVz3jewsU0ZcYsoDGkrn+Zl/5fwq8wBFB0bLLzveKyvORsnGwpNyuqDELDthXXpCC6kHJrSUoZdEmjDSKzas3tejQXN/Zr6s2Jluans2HTlV4+g1DaF/HtNFLusYWc87Rl05NABlZWVJaWmpREVVHuqnz3ft2lXt52idUHXH637VtWtXadeunQlVL730kuny+vvf/y7Hjh2T48fL+1bPp8dWDFXWFqCm0Opc3c/Law6YZs9b+rdlNBgAu9E34eTjOfJlcrp8vDlVDmblm/2Bfj5ye2I7+VnvNmaE1dq9mWb2YR2V+smWVLOpbm1C5YrOkeZ3lbbWZOQUmo86UklbLc6aQuQyKT5bZgqUw5o3M3Oc6VqH2sWffabEtO7o61Y6wV+bsAATrlqH+Jvt8k6R0iPGvq0eGoJGxI2odiboSf3KC66TU3Nk0+FTJpz8cPCkubdv950wm9JCZL0+nbhWt1YhfhLk52valDTM6PfXmmn0V7d2p3l7aZ1SqXy7L0sOnSiocl1axxQc4GvOE9MiQCYP6yBXd49yeFecp3F4DZC9ae3Pv//9b7nrrrskPDxcfHx8TIvS2LFjzT/U6mjrkG6OcEv/WPls23FJzS6UBz/cKi+tOSAPjO4sY3pE858BQL1o2Pj+4An5KjldvtqZISmnz9he04By59AEs1kX2ezXrqVMHJpgPi/pyCkThtbsyZJtKdmy83iO2WpLa3gqfj0rndFYu/k1cHVo1XSzG2vYqWmoezMfb9PyottvpP25Fpu8cwEoS747cEJyCs/KsVNnzFYffj7eMrhDuJnT58ourSQqNMC0MvH73cMDUGRkpAkoOmqrIn2uNTvV0f0XO37AgAGyefNm0weo3WytWrUytUYDBw4UZ6OjG1Y/OELeWn9I/rFqv+zLyJPf/TPJ9Ee/+ZtE0yoEABejLS2rdmeYlp7VuzMlt+inoeE6R47W+FzdLUqu691Ggmr4vaKtHZd2iDDbg2PKu+a1y+qbvVlmdJVprQnVVhB/aRUcIIH+PuYNXoOEzsmjb+o6pYfObq8TvJ7ILzb1NjqiqUtUiNO/6ev1dWwdYjYNhNrCk3LqjGTmlbd4mS23WAqKz5o6KR/T2mM+sfwE51qDys79sa3dZ8M6t+L3uJNyiiJoHfKuw9StBcvahXXPPffUWARdUFAg//nPf2z7hg4dKr179zZF0NXZu3ev6RpbtmyZjB492q5FVPak/eTaFfbqNweloLhUnripp/zq0vgm+/oAnJu+Id/7f0myanembdI/DTcaQLRr62yF+hLtrhnVrbXpWtG6ksYq7AWcicsUQSutvZk4caJpndEgpMPgdZTXpEmTzOsTJkyQ2NhYU6is7rvvPhk+fLg888wzct1118l7770nGzdulMWLF1eaJ0hbfTRIbdu2zXyODo2vTfhxpNCAZnL/6C6if0s89/U+0xRNAAJg9dKa/fLZtvJ6R/0jSaSkSjeTBh7d+rZt0aijuQBX5/AApC06mZmZMnPmTFPIrMPZly9fbit0PnLkiBnFVbG1R+f+efTRR+Xhhx82kxvqCDCdA8hKi501WGnXWJs2bUyI0okTXUW/+PLZLayTgQHA5qOnZf4Xe8zjx2/oYUYolU8AqJMBlklMi+bSPtK55jkDnJnDu8CckaO6wKy0D73vX780j5Meu9q2ZAYAz6SFxdc9t1YOnygwRcTP397P6etpAGd//2ahESfUItBPOrQq/0tu89FTjr4cAA428+PtJvzEtmguT97ci/AD2AEByEnpjKRKV0MG4NlLRPw7KcWMNnr2F33NMHYADUcAcvYAdIQWIMBT6Zw0uj6W+uPITjIwIdzRlwS4DYcXQaN6uh6N2nL0tBn6yuzQgPvQKS+e/WqvBPn5SN92LaRvXEtbrV9uYYks355mZm1etz/LzCuj62PdM6Kjoy8bcCsEICfVOSrETJ6lxY970nPNdPQAXJ9Ooveb13+QjYcrt+7GRwSaJSJ0ceSKy0Yktg+XBeP72mV9LAA/IQA5KW3x6RMXZqZk124wAhAakw4GfXPdIVm4ar/5t6YjjcZ0j5awQOpN7EmHrP/P25tM+AkN8JVR3aNMK68umKlFzrqpS1oFyU19Y+XGvrHSLqJhq6IDqB4ByMnrgEwAOnxa7hjMhIjuEjQWrtwn3x04KX+6urNZ8sTefjxyyrypaitij9iwixbNZuQWyoMfbDULVqrMXF0HKlMe8dlmlk+4vk8b+VnvGDPbsDN9H3Xm4wOZ+WbdqWOnCsxHXQ7i5n5tZVz/WKcbKXW2tEzuffdHWbs3yyxE+vqkRNvPP7ugRLYcO23uSff1iAl1uusH3A3zADnhPEBWX+9Kl9+8sVE6RAbJ1w9c6bDrgP0sWr1f5izbZR7r+5suSPngmC4S6Odrl9aFeZ/vlle+OWjWX7JKiAiUnrFh0rttmPSKbSE9Y0MlJKA8FOlimX/+11azArgurfDA6C5SWFIq/912XHal5drO0TkqWP73xp4yuEOE2IOGFa1z+Xx7min0Hd65ldw6oK1Zg6qm2Yv1V9WO1BxZtv24LNueZsJPTa7q2lpm39LLLDzpDMrKLHL/B1vkox9TzHpbb9w5SIZ2jHT0ZQEe/f5NAHLiAHQqv1j6/W/5hIg/Pna1beVmuKZ/bTpm3gTVoISW8sOh8hqQuPDmMueW3ma9pvralZYjU9/bbAstg9uHS2r2GTl6svoVrHWeqZiw5mahS9U1OkSeu72faTWy2pueK59uPS5vf3fYBCR1S/9YmTG2m1kMs64OZeXLZ9uPm+Cz9Vh2tcfoPDf6NYZeEinZZ4olM69YMnOLJCOnUNbtPyFHTpZ3ESldhLNzdLC0bREosS2bS9uWzeVUQYksWrVfikvLTBfTX27oITf3c1xrkLb6aMvOW+sPm6JmX28vWfSrAabrC4D9EYDcJACpq+atkgNZ+fL6nYNkRNfWFzxWf5QbDp6UbjGhZl0xOA9dpfu3b240i1XefUUHefjabmafDnHW1hB1ZZdWZikDDSZtWgRIm7Dm0r1NqDT387lgy8Jr3x6Uvy3fbd70I4L8ZM643mYtKGuI3p6abQLHNt1Ssm1fz2rysPbywJguZmHNmmYm/9vnu+X/NhwxLUshAb4yeVgHaRMWYK6teTMf81H/zWlI12uwLrx5IDNPPtt2XP67LU12Hs+xnVMbeQYlhMs1PaNN6NJjPtmSKrmFP61gXh1d+PPKzq1lbK9o08pjbcmqSAcNPPDBFlvIGtm1tVlTT1uXLvS9tBcNa18kp8vavZmybt8J26rsmsGe/UU/uaFPTKNfA+CpcghA7hOA7n9/i/wr6Zjce1VHs1Dqhbz/w1HTnaF/zf/7D0Pt0q2CmoOHBokzJaXmr3pfb2/x9fEydTI6nLnitAVbj52WXyz+zixeeVPfGJn/8762bh4d5ff0sl2mlaU6Wr/zy8HtTFdZxe4cHUn0r6QUef3bg7auIH2j1/BzsdaZrLwiE4T2pOWaepPazi2jtUWPLt1uuqEuRmtcdBRjRm6RbZ9+T3T9qmt7tTEBTVcrr0i73r5MTjf/3vWeIoP9zDF6P7p1iQqR4V1a1erftba8vLTmgCz4ao+UlJb/itOuJw1BI7q0MiOr9JzhgX52G12lQ9tfWr1fXv3moFmby6pFYDO5vGOk3DYwznT1AWg8BCA3CkD//O6wedO5rGOEvPPbS2s8Tt8Qh89dZboLFOsFVab/zHWkzfaUbNNS4uPlZQKLt1d5aNE3W+sbrfUNVt9Es/KKJT2n0Gw6Qmd3eq5pYdibnmfCT3U0EEWHBZgumdgWgaal50R+sXkTfO3OQeaN+Hx6XRsPnZTj2YWSml0oadln5GBWgQkrqpmPl1zfO0ZuHdhW1uzJMq0xWvCrQvx9Zca13eT2xLhG/3nrnFTv/XBEvt2XZQLdGd1KSs3jnDMlcqqg2BY4rN8LrXW5rle0jO4e3eTduPqzemPdIVm9O7NKy1fFkBkR7Cfx4YHSr11LM/hAR2BW17pUHR2yrj+PZ1fstXUVar3V6O5RckXnVtIjJox5vIAmQgByowCUnJoj1z631vw1vWXW6Bp/kT63Yq/M/3KPeQPXbg/tann42q5y9xWXiKvTN5gyS/lkkBpY9FtQmzd6bXnRGpekw6dk0+FTpj6kNvR7rV0tGlou9L9Dg4weW1JaJmdLLSYcaLiqjhYev3f3EHN8ben5VuxMl1fWHpQNh05WeV3njJl0WYJpWajLeRuT/jrRLh/9N6jfbw0VzlC7pte1LyNPVu7OkFW7M02tlIa1mn6++s9LW5xCmzczPwcNw/p/Sh/7N/ORAF9vW/efdu0dOjd8XWurpl/T1bRw8ccH0PQIQG4UgPQXbu+/fC75xaWyfOow6Rpd9Xq01efKuSvNMdrqozUbj328wwSFN3+TaIYyuyL9pzn3893y8toDlVoVVMvAZmbU0IQhCRIXHljpc3TqgOe+3mvqoSrSUU69YvUve1/zZqahSr+/RWfL5EResRkOXrHrQmnoah3ib7a2LQNNvUqX6GDzMT4iqEog1fPpeVJO6dDs8uHZeUWl8tth7at0+dSFDmvXrhUNRDqi667L28vIblG0LDSA/qz0/4oGXW1p252WK0lHTpvAXFNrUU20BXHqqM4yflCcU00XAHiaHAKQ+wQg9cuXvzMjYJ66uZepBznfY0u3mxqSPm3D5KM/XGb+ev3zh1vlg03HTP3Bf+65vFJIaKo3Fy2A1XoR7d5JPp5j9vWNa1HezRDfQlqH1DxEWf/ifvijbfL+xmMX/Dp6r1r7MnFogmmF0eDz45HTtlFCI7q2MsW25XOrhFXb/WSl/xU0RGqg1K4dbU3Tgt6ahmXDfWkh85Zj2ab1UUOmdkHqR910n3b76b8RrVvS4vFre7dxmlY4wJPl1OH9m/+xLrIumAYgnRH6/ACkc6i8u+GIeTx9bDfbm/X/3tRT9mTkmZaDyW9tbNKi6BdX7TddctXVyOg0/1ZaIzOqW5TpxtHWFCt9U9Eh3ct3pJlWLB0irqN+dE2ksnMtNzq0+I11h82EfV/tzDBbxZae2xPbyf8M15FKzWt93dploW9ivJGhdWiAXN3dOeYQAtA4+E3vQivD6yic881dvtu0rOiQ4CGX/DRJnQ5DXvSr/nL989+YeocHP9wqLzRBUfTKXRny9PJdtpFAuqxCz5hQ0/oiXuX3oC00WkysXURaoPrW+kNmOLQOre4UFSJ3v7XRBD5twXn+l/1kTI/oKl/nqq5RZtMA+Pb6w/LhpmMmGOlwZ+1uulDrEgAAdIG5QBeYjizpf25CxGGdIs0wYh1hooWX415cZ1pJlt13hXSJ/mkSO6sfDp00XWhaQ6MzDk9pxBWltdtg7LNrTU2FDtt+7Gfda6xR0RWv9dp0gjgtSrXSLrvTBSVmleyXJwys9Wy52i2h2Y76CwDwXDnUALlXAFL3vJtkZuW10lyhI1Q0LIwfGCdP39q7xs999/sjpp5GA8LLvx7YKLPQatfUxNc3mHWOtNVn6ZShNU6sdz4tPn1l7QFZujnFBDUtcH5jUqL0iWth9+sEALgvApAbBiClCyWadZC2pZmJ7JQO1171wAgz78yFWAultb7loz8MNV1N9rR4zX556rNd5no+vfdy6di67ufXuXY+35FmZvplBWwAQF0RgNw0AFV09GSBmdNE5yqpzQKVOlfNr1/93qxCHh8RKB9PuUxaBNZxfpayUpHD60Ty0kWCo0Tih4p4+5glFm558VvTelPTSDUAABobAcgDAlB9a4lueOEbU3ysM0vrkgy1Xi07+ROR5Q+J5KT+tC80RgpHPSVjv2hpWqeu6REtL/6qPxPAAQAcggDUQO4agJTOWquF07p0geaUyy6JlJv6xZpRWDUO/9bw8/4EnSmn0m6LDusSi/yueKpsDblClt03rO6tSgAA2AkBqIHcOQCp9ftPyDNf7JaNh38aVq+1Ozf2iZWZ13eXoIpBSLu9FvSs3PJTgc7Nk+EVIal3bpD+CbUbsQUAgKPfvxkz7IF0vqAPfz9U1jw4Qu6/urN0iAwyS0As2XjUtA5VWgZAa35qCD/W0WjRckL6W3Y2zcUDAGAHBCAPpiOt7h3ZSVbcP1ze/e1gs1aVTpp44wvfmlmnDS14ro3aHgcAgBMgAMEULeuEgx/fc5mZw0cXhvzF4u/k480psi69lpOF66gwAABcBAEINrEtmsuHvxti1ufSmZXve2+z/OorH0m1hEvlNdIr0hkZY8uHxAMA4CIIQKhEC6AX/3qAWUhUeXv7yKZuD4mXGfF1/vD2c8+vmWPmAwIAwFWwGCqq0BXlZ4ztJqO7R0tEkJ8kRAaJJMdUOw+QCT/db3Dk5QIAUGcEINRoQHz5KvSGhpyu11U7EzQAAK6GAITa07DTfpijrwIAgAajBggAAHgcAhAAAPA4ThGAFi5cKAkJCRIQECCDBw+WDRs2XPD4Dz74QLp27WqO79Wrl3z22WeVXs/Ly5N77rlH2rZtK82bN5fu3bvLokWLGvkuAACAq3B4AFqyZIlMmzZNZs2aJUlJSdKnTx8ZM2aMZGRkVHv8unXr5Pbbb5e77rpLfvzxR7npppvMtn37dtsxer7ly5fLP//5T9m5c6dMnTrVBKJPPvmkCe8MAAA4K4cvhqotPoMGDZIXXnjBPC8rK5O4uDi59957Zfr06VWOHz9+vOTn58unn35q23fppZdK3759ba08PXv2NMc99thjtmMGDBggY8eOlSeeeKLKOYuKisxWcTE1vQZ3XQwVAAB35DKLoRYXF8umTZtk1KhRP12Qt7d5vn79+mo/R/dXPF5pi1HF44cOHWpae1JSUkTz3cqVK2XPnj0yevToas85e/Zs8w2zbhp+AACA+3JoAMrKypLS0lKJiqq8jpQ+T0tLq/ZzdP/Fjn/++edN3Y/WAPn5+ck111xj6oyuuOKKas85Y8YMkxat29GjR+1yfwAAwDm55TxAGoC+++470woUHx8va9askSlTpkhMTEyV1iPl7+9vNgAA4BkcGoAiIyPFx8dH0tPTK+3X59HR0dV+ju6/0PFnzpyRhx9+WD766CO57rrrzL7evXvL5s2bZd68edUGIAAA4Fkc2gWm3VNanLxixQrbPi2C1udDhgyp9nN0f8Xj1Zdffmk7vqSkxGxaS1SRBi09NwAAgMO7wHTI+sSJE2XgwIGSmJgoCxYsMKO8Jk2aZF6fMGGCxMbGmkJldd9998nw4cPlmWeeMS087733nmzcuFEWL15sXteqb339wQcfNHMAaRfY6tWr5a233pL58+c79F4BAIBzcHgA0uHqmZmZMnPmTFPIrMPZdQ4fa6HzkSNHKrXm6Aivd999Vx599FHT1dWpUydZunSpGfpupaFIC5vvuOMOOXnypAlBTz75pPzud79zyD0CAADn4vB5gJyRjgRr0aKFGQ3GPEAAALgG6zx+p0+fNtPaOHULkDPKzc01H5kPCAAA13wfv1gAogWoGlosnZqaKiEhIeLl5WXbrzNW//DDD/V6bn1sTaf2aF06/+vV97iaXud+uV93ud/a7ON+Pft+dXCNve61NvfhTvc7qJb32hT3q5FGw49Oe3P+YKjz0QJUDf2m6SSK59ORZBX/odTl+fmv6eOG/qM7/5z1Pa6m17lf7tdd7rc2+7hf7tde91qb+3Cn+/Wp5b021f1erOXHaRZDdSU6mWJ9n5//WmNcT32Pq+l17pf7dZf7rc0+7pf7tSdPut8pdThfU91vbdAF5sQLtbkD7te9cb/uzZPu15Pu1RPvtzq0ADUxXXJj1qxZHrP0Bvfr3rhf9+ZJ9+tJ9+qJ91sdWoAAAIDHoQUIAAB4HAIQAADwOAQgAADgcQhAAADA4xCAAACAxyEAOandu3dL3759bVvz5s3Nqvfu7ODBgzJixAjp3r279OrVS/Lz88WdJSQkSO/evc3PV+/bExQUFEh8fLw88MAD4s50IcaBAwean23Pnj3l5ZdfFnemyylceeWV5v+u/pv+4IMPxN3dfPPN0rJlS7n11lvFHX366afSpUsX6dSpk7zyyivijhgG7wLy8vLMm+Xhw4clKChI3NXw4cPliSeekGHDhsnJkyfN5Fy+vu67Wov+TLdv3y7BwcHiKR555BHZt2+fWYNo3rx54q5KS0ulqKhIAgMDTZDXELRx40aJiIgQd3T8+HFJT083gS8tLU0GDBgge/bscevfV6tWrTJrTr355pvy4Ycfijs5e/asCbMrV640kyXqz3PdunVu9++XFiAX8Mknn8jIkSPd+pfJjh07pFmzZib8qPDwcLcOP55o7969smvXLhk7dqy4O12XSMOP0iCkf2e689+abdq0MeFHRUdHS2RkpPkjxp1pi5cumO2ONmzYID169JDY2FjzB5r+n/3iiy/E3RCA6mnNmjVy/fXXmxVndcX46rqnFi5caP7KDwgIkMGDB5t/VPXx/vvvy/jx48Wd71ffHPU/mn6N/v37y1NPPSXu/vPV82qrl65i/M4774i73692e82ePVucQVPcr3aD9enTxyys/OCDD5pQ4Am/rzZt2mRawLSVzxPu1xk19P5TU1NN+LHSxykpKU12/U2FAFRP2qytv9z0H1F1lixZItOmTTNTjSclJZljx4wZIxkZGbZjrPUB52/6j6/iei3a9HjttdeKO9+vNrmuXbtW/vGPf8j69evlyy+/NJs7/3y/+eYb82ahLXwa+LZu3Sruer8ff/yxdO7c2WzOoCl+vi1atJAtW7aY2rZ3333XdBG5++8rbfWZMGGCLF68WBypqe7XWdnj/j2C1gChYfTb+NFHH1Xal5iYaJkyZYrteWlpqSUmJsYye/bsOp37rbfestxxxx0Wd7/fdevWWUaPHm17/re//c1s7v7ztXrggQcsr7/+usVd73f69OmWtm3bWuLj4y0RERGW0NBQy+OPP27xlJ/v73//e8sHH3xgcef7LSwstAwbNsz8znImjfnzXblypWXcuHEWZ1af+//2228tN910k+31++67z/LOO+9Y3A0tQI2guLjY/GU/atQo2z5vb2/zXFs3XK37qynuV7uB9K+PU6dOSVlZmWnC7datm7jr/epfaFpAaS1y//rrr02fu7ver3Z96UihQ4cOmeLnyZMny8yZM8Vd71dbe6w/X11tW/8964gad71ffZ+988475aqrrpJf//rX4im/n11Rbe4/MTHRDNDQbi/9/bRs2TLTQuRuqDJtBFlZWaYPPCoqqtJ+fa5FoLWlvzi1X/Zf//qXuPv9asGzdgNdccUV5pfp6NGj5Wc/+5m46/3qG6QOo1V6Lg0EGgLd+d+zq7DH/eqIzbvvvttW/HzvvfeaqR3c9X6//fZb062iQ+Ct9SZvv/22U96zvf49a2DQLk79Y0brvHTo/5AhQ8TZ1eb+fX195ZlnnjHTc+gfpH/+85/dbgSYIgA5MR1+6Mi6gaamIw08YYSQ6tChg/nl6Ym0pcDd6V/QmzdvFk9x+eWXmzdKT/LVV1+JO7vhhhvM5s7oAmsEOtpDh8GeH170uQ4RdTfcbznu1z1wv+W4X/fk6fdfEQGoEfj5+ZmJo1asWGHbp38d6XNXaCKtK+6X+3Un3C/36848/f4rogusnrQwTGe0tdKhrtrkrRP4tWvXzgwxnDhxopkOX5vDFyxYYPqKJ02aJK6I++V+uV/u11V42v2ez9Pvv9YcPQzNVenwR/32nb9NnDjRdszzzz9vadeuncXPz88MO/zuu+8sror75X65X+7XVXja/Z7P0++/tlgLDAAAeBxqgAAAgMchAAEAAI9DAAIAAB6HAAQAADwOAQgAAHgcAhAAAPA4BCAAAOBxCEAAAMDjEIAAAIDHIQABcDsJCQlmfSMAqAkBCEC93HnnnXLTTTeJM/rhhx/k7rvvbpKg5eXlZbbAwEDp1auXvPLKK3U+j37+0qVLG+UaAVSPAATAZZSUlNTquFatWplA0hT++te/yvHjx2X79u3yq1/9SiZPnizLli1rkq8NoP4IQAAahQaCsWPHSnBwsERFRcmvf/1rycrKsr2+fPlyufzyy6VFixYSEREhP/vZz2T//v221w8dOmRaRpYsWSLDhw+XgIAAeeedd2wtT/PmzZM2bdqYz50yZUqlcHR+F5ieR1tmbr75ZhOMOnXqJJ988kml69Xnul+/zogRI+TNN980n3f69OkL3mdISIhER0dLhw4d5KGHHpLw8HD58ssvK7VGXX311RIZGSlhYWHmXpKSkipdq9Jr069nfa4+/vhj6d+/v7kmPf/jjz8uZ8+ercdPA8D5CEAA7E5Dw1VXXSX9+vWTjRs3mrCTnp4uP//5z23H5Ofny7Rp08zrK1asEG9vbxMCysrKKp1r+vTpct9998nOnTtlzJgxZt/KlStNWNKPGlTeeOMNs12Ihgf9+lu3bpVrr71W7rjjDjl58qR57eDBg3LrrbeaYLVlyxb5n//5H3nkkUfqdM963f/617/k1KlT4ufnZ9ufm5srEydOlG+++Ua+++47E7L06+t+a0BSr7/+umlJsj5fu3atTJgwwdx7cnKyvPTSS+Yen3zyyTpdF4AaWACgHiZOnGi58cYbq33tf//3fy2jR4+utO/o0aMW/ZWze/fuaj8nMzPTvL5t2zbz/ODBg+b5ggULqnzd+Ph4y9mzZ237brvtNsv48eNtz/X1v//977bnep5HH33U9jwvL8/sW7ZsmXn+0EMPWXr27Fnp6zzyyCPmmFOnTtX4PdCv4+fnZwkKCrL4+vqa48PDwy179+6t8XNKS0stISEhlv/85z+Vru+jjz6qdNzIkSMtTz31VKV9b7/9tqVNmzY1nhtA7dECBMDutBVFW2e0+8u6de3a1bxm7ebau3ev3H777aZrJzQ01Nb1c+TIkUrnGjhwYJXz9+jRQ3x8fGzPtSssIyPjgtfUu3dv2+OgoCDzNa2fs3v3bhk0aFCl4xMTE2t1rw8++KBs3rxZvv76axk8eLD8/e9/l44dO9pe15YvrQvSlh/tAtOvm5eXV+U+q/sean1Rxe+hnkdbiQoKCmp1bQBq5nuB1wCgXvQN/vrrr5enn366ymsaVpS+Hh8fLy+//LLExMSYLqSePXtKcXFxpeM1rJyvWbNmlZ5r7cz5XWf2+Jza0NoeDTy6ffDBB2YkmIa27t27m9e1++vEiRPy7LPPmvv19/eXIUOGVLnP6r6H2m13yy23VHlNa4IANAwBCIDdaeGu1sNoq46vb9VfMxoItNVFw8+wYcPMPq2RcZQuXbrIZ599VmmftRanLuLi4mT8+PEyY8YMU8Csvv32W/nHP/5h6n7U0aNHKxWDW8NZaWlple+hfo8qtiYBsB+6wADUW3Z2tun+qbjpG7yOytICY+3i0iCh3V6ff/65TJo0ybzRt2zZ0ozeWrx4sezbt890H2lBtKNo0fOuXbvMKK49e/bI+++/byuq1paiutCi5f/85z+muFtp19fbb79tiri///57U3zdvHnzSp+jQVELwdPS0kwRtZo5c6a89dZbphVox44d5vPfe+89efTRR+1234AnIwABqLdVq1aZkV4VN33D1i4tbfnQsDN69GjTLTR16lQz5F1He+mmb+abNm0y3V5/+tOfZO7cuQ67j/bt28uHH34o//73v02t0IsvvmgbBaZdVnWhXV96zxpg1KuvvmpCjbbo6FQAf/zjH6V169aVPueZZ54xQ+e1BUm/h0pHvH366afyxRdfmPqkSy+91NQXaTcagIbz0kpoO5wHANyKDjdftGiRadEC4H6oAQIAEVOnoy0t2jWnrVfaInXPPfc4+rIANBICEACcG5b/xBNPmNqldu3ayf3332+KmQG4J7rAAACAx6EIGgAAeBwCEAAA8DgEIAAA4HEIQAAAwOMQgAAAgMchAAEAAI9DAAIAAB6HAAQAAMTT/D8Qag0pu/NtywAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(\n",
    "    dls, model, loss_func=loss_func, metrics=metrics, cbs=[CSVLogger()]\n",
    ").to_fp32()\n",
    "\n",
    "# Choose optimizer and hyperparameters (FastAI will create Adam by default)\n",
    "# You can override like this:\n",
    "learn.opt_func = partial(Adam, wd=1e-2)\n",
    "# Find a good learning rate\n",
    "lr_min, lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))\n",
    "print(\"Suggested LRs:\", lr_min, lr_steep)\n",
    "lr = float(lr_min)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lrfind_helper_subset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: LR finder with capped iterations for small subsets\n",
    "# lr_min2, lr_steep2 = learn.lr_find(\n",
    "#     suggest_funcs=(minimum, steep), num_it=min(100, len(dls.train))\n",
    "# )\n",
    "# print(\"Subset-safe Suggested LRs:\", lr_min2, lr_steep2)\n",
    "# # Optionally override lr for training below\n",
    "# lr = float(lr_min2)\n",
    "# lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6986d00",
   "metadata": {},
   "source": [
    "## Fit One Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9412b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>ccc_v</th>\n",
       "      <th>ccc_a</th>\n",
       "      <th>ccc_avg</th>\n",
       "      <th>mse_fe</th>\n",
       "      <th>mae_fe</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.086080</td>\n",
       "      <td>0.085693</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>-0.009432</td>\n",
       "      <td>-0.002272</td>\n",
       "      <td>3.084938</td>\n",
       "      <td>1.501967</td>\n",
       "      <td>06:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.085958</td>\n",
       "      <td>0.085655</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>-0.009424</td>\n",
       "      <td>-0.002257</td>\n",
       "      <td>3.083589</td>\n",
       "      <td>1.501558</td>\n",
       "      <td>06:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(EPOCHS, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673028d",
   "metadata": {},
   "source": [
    "Spearmanâ€™s Ï: Only calculated during test evaluation rather than as an epoch metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0c2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = 6.0  # both V and A divided by 6 for unit space\n",
    "SCALE_SQ = SCALE * SCALE  # 36.0\n",
    "\n",
    "\n",
    "def mse_fe_to_unit(mse_fe):\n",
    "    \"Convert FE-space MSE (original units) to unit-space [0,1] MSE.\"\n",
    "    return float(mse_fe) / SCALE_SQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ffa7dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08565525"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_fe_to_unit(3.083589)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c535672",
   "metadata": {},
   "source": [
    "# Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7791d827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/desmondchoy/Projects/emo-rec/.venv/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float32). Setting the correlation coefficient to nan.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.08630147576332092,\n",
       " 'test_ccc_v': nan,\n",
       " 'test_ccc_a': nan,\n",
       " 'test_ccc_avg': nan,\n",
       " 'test_mse': 3.1068525314331055,\n",
       " 'test_mae': 1.4975417852401733,\n",
       " 'test_mae_v': 1.5905790328979492,\n",
       " 'test_mae_a': 1.404504418373108,\n",
       " 'test_mae_avg': 1.4975417256355286,\n",
       " 'test_spearman_v': 0.012456890195608139,\n",
       " 'test_spearman_a': -0.06960919499397278,\n",
       " 'test_spearman_avg': -0.02857615239918232}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your df_test actually contains the target columns:\n",
    "test_dl = dls.test_dl(df_test, with_labels=True)\n",
    "test_metrics = learn.validate(dl=test_dl)\n",
    "\n",
    "names = [\n",
    "    \"test_loss\",\n",
    "    \"test_ccc_v\",\n",
    "    \"test_ccc_a\",\n",
    "    \"test_ccc_avg\",\n",
    "    \"test_mse\",\n",
    "    \"test_mae\",\n",
    "]\n",
    "# Handle None values in metrics\n",
    "safe_metrics = [float(m) if m is not None else 0.0 for m in test_metrics]\n",
    "res_basic = dict(zip(names, safe_metrics))\n",
    "\n",
    "# Full-dataset MAE and Spearman's rho on FE units\n",
    "preds_ref, targs_ref = learn.get_preds(dl=test_dl)\n",
    "preds_fe = ref_to_fe(preds_ref.cpu())\n",
    "targs_fe = ref_to_fe(targs_ref.cpu())\n",
    "\n",
    "# MAE per-dimension and average\n",
    "abs_err = torch.abs(preds_fe - targs_fe)\n",
    "mae_v = abs_err[:, 0].mean().item()\n",
    "mae_a = abs_err[:, 1].mean().item()\n",
    "mae_avg = (mae_v + mae_a) / 2.0\n",
    "\n",
    "\n",
    "def _compute_spearman(x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    x = x.detach().cpu().flatten()\n",
    "    y = y.detach().cpu().flatten()\n",
    "    try:\n",
    "        from torchmetrics.functional import spearman_corrcoef as tm_spearman\n",
    "\n",
    "        val = tm_spearman(x, y)\n",
    "        return float(val.cpu())\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from scipy.stats import spearmanr\n",
    "\n",
    "        r = spearmanr(x.numpy(), y.numpy()).correlation\n",
    "        return float(0.0 if np.isnan(r) else r)\n",
    "    except Exception:\n",
    "        pass\n",
    "    import numpy as np\n",
    "\n",
    "    def rankdata(a: np.ndarray) -> np.ndarray:\n",
    "        sorter = np.argsort(a)\n",
    "        inv = np.empty_like(sorter)\n",
    "        inv[sorter] = np.arange(len(a))\n",
    "        a_sorted = a[sorter]\n",
    "        obs = np.r_[True, a_sorted[1:] != a_sorted[:-1]]\n",
    "        group = obs.cumsum() - 1\n",
    "        counts = np.bincount(group)\n",
    "        cums = np.cumsum(counts)\n",
    "        starts = cums - counts + 1\n",
    "        avgranks = (starts + cums) / 2.0\n",
    "        return avgranks[group][inv]\n",
    "\n",
    "    xn = x.numpy()\n",
    "    yn = y.numpy()\n",
    "    if xn.std() == 0 or yn.std() == 0:\n",
    "        return 0.0\n",
    "    rx = rankdata(xn)\n",
    "    ry = rankdata(yn)\n",
    "    rx = rx - rx.mean()\n",
    "    ry = ry - ry.mean()\n",
    "    denom = np.sqrt((rx**2).sum()) * np.sqrt((ry**2).sum())\n",
    "    return float(0.0 if denom == 0 else (rx @ ry) / denom)\n",
    "\n",
    "\n",
    "rho_v = _compute_spearman(preds_fe[:, 0], targs_fe[:, 0])\n",
    "rho_a = _compute_spearman(preds_fe[:, 1], targs_fe[:, 1])\n",
    "rho_avg = (rho_v + rho_a) / 2.0\n",
    "\n",
    "{\n",
    "    **res_basic,\n",
    "    \"test_mae_v\": mae_v,\n",
    "    \"test_mae_a\": mae_a,\n",
    "    \"test_mae_avg\": mae_avg,\n",
    "    \"test_spearman_v\": rho_v,\n",
    "    \"test_spearman_a\": rho_a,\n",
    "    \"test_spearman_avg\": rho_avg,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea88ab2",
   "metadata": {},
   "source": [
    "**Root cause**: CCC uses Pearson correlation; if predictions or targets have nearâ€‘zero variance, CCCâ€™s denominator goes to zero and torchmetrics returns NaN with a warning.  \n",
    "\n",
    "Above numbers corroborate this: test_loss 0.0828 in [0,1] space corresponds to test_mse â‰ˆ 0.0828Ã—36 = 2.98 in FE units, so scaling is fine; CCC NaN implies the modelâ€™s preds on the test set are (nearly) constant in at least one dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0a08c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08630145920647515"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_fe_to_unit(3.1068525314331055)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb701d7",
   "metadata": {},
   "source": [
    "## Save head weights (state_dict) and full model if desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "868f961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Path('checkpoints/dinov3_head.pth'), True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_DIR = Path(\"./checkpoints\")\n",
    "SAVE_DIR.mkdir(exist_ok=True)\n",
    "head_path = SAVE_DIR / \"dinov3_head.pth\"\n",
    "torch.save(model.head.state_dict(), head_path)\n",
    "\n",
    "# Optionally export the whole fastai Learner\n",
    "# learn.export(SAVE_DIR/\"learner.pkl\")\n",
    "\n",
    "head_path, head_path.exists()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c304428",
   "metadata": {},
   "source": [
    "# Inference helper (returns V,A in FE units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15879bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: /Users/desmondchoy/Projects/emo-rec/data/Run_2/Violent seniors anniversary/NEWS_180519780_EP_-1_ZZVDLTYNMCGY.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorImage([-0.1416,  2.6164])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "REPO_ROOT = (DATA_ROOT.parent).resolve()  # absolute repo root\n",
    "\n",
    "\n",
    "def resolve_from_root(p):\n",
    "    p = Path(p)\n",
    "    return p if p.is_absolute() else (REPO_ROOT / p)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_image(img_path: str | Path):\n",
    "    img_path = Path(img_path)\n",
    "    if not img_path.is_absolute():\n",
    "        img_path = resolve_from_root(img_path)  # resolve exactly once\n",
    "    if not img_path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "    img = PILImage.create(str(img_path))\n",
    "    x = HFProcessorTransform(processor, target_size=(W, H))(img)\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    out_ref = model(x)\n",
    "    out_fe = ref_to_fe(out_ref.cpu())\n",
    "    return out_fe.squeeze(0)\n",
    "\n",
    "\n",
    "# Quick smoke test: choose first existing path and run\n",
    "for p in df_test[\"image_path\"].tolist():\n",
    "    p_abs = resolve_from_root(p)  # becomes absolute\n",
    "    if p_abs.exists():\n",
    "        print(\"Using:\", p_abs)\n",
    "        display(predict_image(p_abs))  # predict_image wonâ€™t re-resolve absolute paths\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"No existing paths found in df_test['image_path'].\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
